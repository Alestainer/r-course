--- 
title: "R for Data Analysis and Visualization"
subtitle: |
  | ECON 396 (Fall 2017)
  | TR 10:30-11:45, Webster Hall 112
author: "Jonathan Page"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: 
  bookdown::gitbook:
    config:
      toc:
        collapse: section
    highlight: kate
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: jonpage/r-course
description: "This is a course on using R focused on data analysis and visualization through case studies."
---
```{r include=FALSE, cache=FALSE}
rm(list = ls(all = TRUE))
knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, cache = TRUE)
```
# Syllabus {-}

## Office Hours {-}
Monday 2-3 PM and Tuesday 3-4 PM, or by appointment, Saunders 509, jrpage at hawaii dot edu.

## Student Learning Objectives {-}

1. To be familiar with standard techniques for visualizing data, including heat maps, contour plots, (look at the list in the Data Visualization book)
2. To be able to transform raw data into formats suitable for analysis
3. To be able to perform basic exploratory analysis
4. To be able to create data visualizations

There is no prerequisite for this course.

## Resources {-}

### Required {-}

Introductory Statistics with Randomization and Simulation: Available as a free PDF (https://www.openintro.org/stat/textbook.php?stat_book=isrs) or for $8.49 on Amazon.

R Graphics Cookbook

### Recommended: {-}

[RStudio Cheat Sheets](https://www.rstudio.com/resources/cheatsheets/)

## Course Requirements {-}

Grades for this course will be based on weekly assignments (30%), project assignments (30%), the project proposal (5%), the final project deliverable (15%), and final project presentation participation (20%).

### Weekly assignments (30%) {-}

Weekly assignments are short R excercises. Each exercise should take no longer than 15 minutes. You will typically be given time to complete the exercise in class the day the assignment is given. The assignment will be in the form of R Markdown file (*.Rmd). You will submit the completed assignments via [classroom.google.com](https//classroom.google.com) by the following class period. 

## Individual Project {-}

### Project assignments (30%) {-}

Each week, leading up to the project proposal, you will be given an assignment that is designed to provide you with an organized workflow for approaching new data science projects. Project assignments are submitted via [classroom.google.com](https//classroom.google.com), with the exception of the two presentations

### Project proposal presentation (5%) {-}

This presentation should be less than 2 minutes. You simply need to communicate the core question your project seeks to answer and the dataset(s) you will be using to answer this question.

### Final project (15%) {-}

The final project will be an R Markdown document which communicates your project question, the data you used, and your results. 

### Final project presentation participation (20%) {-}

Your final project participation grade is based on a combination of your own presentation and the feedback you provide to your classmates.

## Schedule {-}

The following schedule is tentative and subject to change.

### Week 1 {-}

* **R** [Intro to R and RStudio; Histograms, scatterplots, summary statistics](#intro)
  * **Data** R Sample Datasets
* **Topic** [Data sources overview](#data-sources)
* **Project Assignment** Indentify interesting datasets and questions

### Week 2 {-}

* **R** [read_csv, dplyr basics, heatmaps, hexbins](#read-data)
  * **Data** ACS PUMS *[CSV]*
* **Topic** [Anscombe's Quartet](#anscombe)
* **Project Assignment** Choose question and dataset for your project

### Week 3 {-}

* **R** [ggplot facets, bubble plots, transparency](#facets-and-bubbles)
  * **Data** Hawaii Tourism Authority *[Excel]*
* **Topic** Effective Data Visualization
* **Project Assignment** Write description of your question

### Week 4 {-}

* **R** [geom_smooth, abline, vline, hline](#lines)
  * **Data** State of Hawaii Department of Business, Economic Development (DBEDT) *[Excel]*
* **Topic** Time series analysis
<!-- -->
* **Project Assignment** Write description of your dataset(s)

### Week 5 {-}

* **R** [ggplot2 Extensions and Scatterplot Matrices (GGally)](#ggplot-exts)
  * **Data** ACS Immigration *[CSV]*
* **Topic** [JunkCharts Trifecta Checkup](#trifecta)
* **Project Assignment** Create 2 descriptive plots of your datasets(s)

### Week 6 {-}

* **R** [Boxplots, violin plots](#boxplots-and-violins)
  * **Data** SSA *[Excel]*
* **Topic** 
* **Project Assignment** Write a description of the data cleaning required for your project

### Week 7 {-}

* **R** [Spatial Visualizations with geom_spoke, gganimate, and GGally::glyphs](#geom_spoke)
  * **Data** NOAA Wind *[netCDF]*
* **Topic** 
* **Project Assignment** Write a description of your planned approach

### Week 8 {-}

* **R** [geom_area, geom_ribbon](#area-and-ribbons)
  * **Data** BLS American Time Use Survey (ATUS) *[TSV]*
* **Topic** Project Proposal Description
* **Project Assignment** Work on project proposal presentation

### Week 9 {-}

* **R** [jitter, rug, aesthetics](#jitter-rug)
  * **Data** IPUMS
* **Project Assignment** Present project proposal (<2 Minutes)

### Week 10 {-}

* **R** [Themes, Labels, and Colors](#themes-labels-colors)
  * **Data** PSID
<!-- PSID -->
* **Topic** 
* **Project Assignment** Work on final project

### Week 11 {-}

* **R** polar coordinates, ggradar extension
<!-- should use monthly data for multiple years -->
* **Topic** 
* **Project Assignment** Work on final project (cont.)

### Week 12 {-}

* **R** word/text analysis
<!-- -->
* **Topic** 
* **Project Assignment** Work on final project (cont.)

### Week 13 {-}

* **R** gganimate
<!-- -->
* **Topic** 
* **Project Assignment** Work on final project (cont.)

### Week 14 {-}

* **R** git and GitHub for R
* **Topic** 
* **Project Assignment** 

### Week 15 {-}

* **R** networks, geomnet extension
<!-- -->
* **Topic** Final Project presentations
* **Project Assignment** 

## Other Resources {-}

There are many useful resources you should be aware of while going through this course:

[RStudio's List of Useful R Packages](https://github.com/rstudio/RStartHere)

### Statistics {-}

[Variance Explained](http://varianceexplained.org/RData/resources/)

[R for Data Science - Grolemund and Wickham](http://r4ds.had.co.nz/)

### Visualization {-}

[FlowingData]()

[Junk Charts](http://junkcharts.typepad.com/)


### Courses {-}

[Gary King - Quantitative Research Methodology](http://projects.iq.harvard.edu/gov2001/)

[John Stasko - Information Visualization](http://www.cc.gatech.edu/~stasko/7450/)

[Jenny Bryan - Data wrangling, exploration, and analysis with R](http://stat545.com/)

### Books {-}

[Econometrics in R](https://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf)

[Using R for Data Analysis and Graphics](ftp://cran.r-project.org/pub/R/doc/contrib/usingR.pdf)

### Papers {-}

[Embedded Plots](http://vita.had.co.nz/papers/embedded-plots.pdf)

# Individual Project {-}

## Project assignments {-}

Each week, leading up to the project proposal, you will be given an assignment that is designed to provide you with an organized workflow for approaching new data science projects. Project assignments are submitted via [classroom.google.com](https//classroom.google.com), with the exception of the two presentations

## Project proposal presentation {-}

This presentation should be less than 2 minutes. You simply need to communicate the core question your project seeks to answer and the dataset(s) you will be using to answer this question.

## Final project RMarkdown {-}

The final project will be an R Markdown document which communicates your project question, the data you used, and your results. 

## Final project presentation participation {-}

Your final project participation grade is based on a combination of your own presentation and the feedback you provide to your classmates.

<!--chapter:end:index.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---
```{r include=FALSE, cache=FALSE}
rm(list = ls(all = TRUE))
knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, cache = TRUE)
```

# (PART) R Tutorials {-} 

# R Basics {#intro}

Before we begin, make sure you have [R](https://cran.r-project.org/) and 
[RStudio](https://www.rstudio.com/products/rstudio/download3/#download) installed.

## R Markdown

Throughout this course, [R Markdown](http://rmarkdown.rstudio.com/lesson-1.html) will make our lives easier. Make sure that the `rmarkdown` library is installed:

```
install.packages("rmarkdown")
```

For each assignment, you will create an R Markdown file (*.Rmd) and submit that file by the following class session using [classroom.google.com](https://classroom.google.com)

## Working with data already loaded into R

Base R comes with a set of sample data that is useful for illustrating techniques in R.
Run the following command to see a list of the datasets in the core library `datasets`:

```
library(help = "datasets")
```

These datasets are accessible automatically. We'll start with the Swiss Fertility and Socioeconomic Inicators (1888) dataset. See a description of the dataset by using the help command, either `?swiss` or `help(swiss)`. This dataset is technically a `data.frame`, which you can see by using the command `class(swiss)`. For more information on `data.frame`s take a look at the documentation(`help(data.frame)`)

### Numeric summaries

Here are a few ways we can summarize a dataset:

`head()` shows us the first six rows of a `data.frame`.
```{r}
head(swiss)
```

`summary()` provides summary statistics for each column in a `data.frame`.
```{r}
summary(swiss)
```

### Visual summaries

Scatterplot matrix (default plot of a data.frame):
```
plot(swiss)
# or
pairs(swiss)
```
```{r, echo=FALSE}
plot(swiss)
```

Scatterplot of two dimensions
```
plot(swiss[,c("Education", "Fertility")])
# or
plot(swiss[4,1])
# or
plot(swiss$Education, swiss$Fertility)
# or
plot(swiss$Fertility ~ swiss$Education)
```
```{r, echo=FALSE}
plot(swiss$Fertility ~ swiss$Education)
```

Smoothed Scatterplot of two dimensions
```{r}
smoothScatter(swiss$Fertility ~ swiss$Examination)
```

Scatterplot with a `loess` (locally weighted polynomial regression)
```{r}
scatter.smooth(swiss$Fertility ~ swiss$Agriculture)
```

### Distribution plots

Histograms:
```{r}
hist(swiss$Catholic)
```

Stem-and-Leaf Plots:
```{r}
stem(swiss$Fertility)
```

Kernel density plot (and add a rug showing where observation occur):
```{r}
plot(density(swiss$Fertility))
rug(swiss$Fertility)
```

Boxplots:
```{r}
boxplot(swiss)
```


#### More complicated charts
Conditioning plots:
```{r}
coplot(swiss$Fertility ~ swiss$Examination | as.factor(swiss$Catholic > 50))
```

Star plots (half-star plots here):
```{r}
stars(swiss, key.loc = c(15,1), flip.labels = FALSE, full = FALSE)
```

## Assignment

Choose a dataset from `datasets` (`library(help = "datasets")` will show you a list) and create 5 charts in an R Markdown file from the example charts above. Run the following command to see what else is available in the base R graphics package:

```
demo(graphics)
```

<!--chapter:end:01-intro.Rmd-->

```{r include=FALSE, cache=FALSE}
rm(list = ls(all = TRUE))
knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, cache = TRUE)
```
# Reading data {#read-data}

The first step in analyzing data with R is reading data into it. This lesson focuses on reading data, manipulating it with `dplyr` and a few summary visualizations.

## Data Source:

The US Census Bureau has a large selection of data on the population of the United States. 
The public-use micro surveys (PUMS) are available from the following link:

https://www.census.gov/programs-surveys/acs/data/pums.html

We'll take a look at the 1-year American Community Survey results for the state of Hawaii.

[Hawaii Population Records](https://www2.census.gov/programs-surveys/acs/data/pums/2015/1-Year/csv_phi.zip)

[The data dictionary](https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict15.txt)

## `read_csv`

To read in the downloaded file, we'll use the `readr` package, which you can install by installing `tidyverse`.

```{r}
library(readr) # also in library(tidyverse)
pop_hi <- read_csv("data/csv_phi.zip")
pop_hi
```

## `dplyr`

Using the data dictionary we can identify some interesting variables.

```
PERSON RECORD

RT		1	
	Record Type		
		P .Person Record

AGEP		2	
	Age
		00 	.Under 1 year	
		01..99  .1 to 99 years (Top-coded***)

COW		1	
	Class of worker
		b .N/A (less than 16 years old/NILF who last
		  .worked more than 5 years ago or never worked)
		1 .Employee of a private for-profit company or
		  .business, or of an individual, for wages,
		  .salary, or commissions
		2 .Employee of a private not-for-profit,
		  .tax-exempt, or charitable organization
		3 .Local government employee (city, county, etc.)
		4 .State government employee
		5 .Federal government employee
		6 .Self-employed in own not incorporated
		  .business, professional practice, or farm
		7 .Self-employed in own incorporated
		  .business, professional practice or farm
		8 .Working without pay in family business or farm
		9 .Unemployed and last worked 5 years ago or earlier or never
                  .worked
		
SCHL		2	
	Educational attainment
		bb .N/A (less than 3 years old)
		01 .No schooling completed
		02 .Nursery school, preschool   
		03 .Kindergarten
		04 .Grade 1
		05 .Grade 2
		06 .Grade 3		
		07 .Grade 4
		08 .Grade 5
		09 .Grade 6
		10 .Grade 7		
		11 .Grade 8  
		12 .Grade 9
		13 .Grade 10
		14 .Grade 11		
		15 .12th grade - no diploma   
		16 .Regular high school diploma
		17 .GED or alternative credential
		18 .Some college, but less than 1 year
		19 .1 or more years of college credit, no degree
		20 .Associate's degree		
		21 .Bachelor's degree
		22 .Master's degree
		23 .Professional degree beyond a bachelor's degree
		24 .Doctorate degree
		
WAGP		6	
	Wages or salary income past 12 months
		bbbbbb 	 	.N/A (less than 15 years old)
		000000 		.None
		000001..999999 	.$1 to 999999 (Rounded and top-coded)

Note: Use ADJINC to adjust WAGP to constant dollars.


WKHP		2	
	Usual hours worked per week past 12 months
		bb 	 .N/A (less than 16 years old/did not work 
			 .during the past 12 months)
		01..98   .1 to 98 usual hours
		99 	 .99 or more usual hours
		
WKW		1	
	Weeks worked during past 12 months
		b .N/A (less than 16 years old/did not work 
		  .during the past 12 months)
		1 .50 to 52 weeks worked during past 12 months
		2 .48 to 49 weeks worked during past 12 months
		3 .40 to 47 weeks worked during past 12 months
		4 .27 to 39 weeks worked during past 12 months
		5 .14 to 26 weeks worked during past 12 months 
		6 .less than 14 weeks worked during past 12 months
		
ESR		1	
	Employment status recode
		b .N/A (less than 16 years old)
		1 .Civilian employed, at work
		2 .Civilian employed, with a job but not at work
		3 .Unemployed
		4 .Armed forces, at work
		5 .Armed forces, with a job but not at work
		6 .Not in labor force
		
PERNP		7	
	Total person's earnings
		bbbbbbb 		.N/A (less than 15 years old)
		0000000 		.No earnings
		-010000 		.Loss of $10000 or more (Rounded & bottom-coded 
       					.components) 
		-000001..-009999 	.Loss $1 to $9999 (Rounded components)
		0000001 		.$1 or break even
		0000002..9999999 	.$2 to $9999999 (Rounded & top-coded components)
		
Note: Use ADJINC to adjust PERNP to constant dollars.

PINCP		7	
	Total person's income (signed)
		bbbbbbb 		.N/A (less than 15 years old)
		0000000 		.None
		-019999 		.Loss of $19999 or more (Rounded & bottom-coded 
       					.components) 
		-000001..-019998 	.Loss $1 to $19998 (Rounded components)
		0000001 		.$1 or break even
		0000002..9999999 	.$2 to $9999999 (Rounded & top-coded components)
			
Note: Use ADJINC to adjust PINCP to constant dollars.
```

Let's focus on employed civilians (ESR either 1 or 2) working full time (WKHP > 32) for close to the entire year (WKW either 1 or 2).
```{r}
library(dplyr) # also in library(tidyverse)
pop_hi <- pop_hi %>%
  filter(
    ESR %in% c(1, 2),
    as.numeric(WKHP) > 32,
    WKW %in% c(1, 2)
  )
```

If you are unsure if a column that you want to treat as numeric contains letters, you can run the following
command to get a list of the values containing letters:
```{r}
grep("[[:alpha:]]", pop_hi$WKHP, value = TRUE)
```

We can use two functions to add new columns (or change existing ones).

1. `mutate()` adds columns and keeps the previous columns
2. `transmute()` adds columns and removes the previous columns

This time we want to drop the columns we don't mention.
```{r}
pop_hi <- pop_hi %>%
  transmute(
    age = as.numeric(AGEP),
    worker_class = factor(COW, labels = c(
      "for-profit",
      "not-for-profit",
      "local government",
      "state government",
      "federal government",
      "self-employed not incorporated",
      "self-employed incorporated",
      "family business no pay"
    )),
    school = SCHL,
    wages = as.numeric(WAGP),
    top_coded_wages = WAGP == 999999
)
```

Creating a custom factor variable for educational attainment: 
```{r}
education_levels <- c("less than HS", "HS", "associates", "bachelors", "masters", "doctorate")

pop_hi$education <- NA
pop_hi[pop_hi$school < 16,]$education <- "less than HS"
pop_hi[pop_hi$school > 16 & pop_hi$school < 20,]$education <- "HS"
pop_hi[pop_hi$school == 20,]$education <- "associates"
pop_hi[pop_hi$school == 21,]$education <- "bachelors"
pop_hi[pop_hi$school %in% c(22, 23),]$education <- "masters"
pop_hi[pop_hi$school == 24,]$education <- "doctorate"

pop_hi <- pop_hi %>%
  mutate(education = factor(education, levels = education_levels))
```

## First Look at `ggplot2`

See the [ggplot2 documentation](ggplot2.tidyverse.org/reference/) for details and inspiration.

```{r}
library(ggplot2) # also in library(tidyverse)
ggplot(pop_hi, aes(age, wages)) +
  geom_point()
```

Income has a skewed distribution, so it is often presented/analyzed in logs. Here's how to modify the above 
chart to display income in logs:

```{r}
pop_hi <- pop_hi %>%
  mutate(log_safe_wages = ifelse(wages == 0, 1, wages))
pop_hi %>%
  ggplot(aes(age, log_safe_wages)) +
  geom_point() +
  scale_y_log10()
```

## Heatmaps

Heatmaps allow you to get a sense of the concetration of observations in regions where there are many
overlapping points:

```{r}
ggplot(pop_hi, aes(age, log_safe_wages)) +
  geom_bin2d() +
  scale_y_log10()
```

## Hexbins

Hexbins use hexagons instead of squares, which helps avoid the rectangular sections in
heatmaps that may misrepresent your data.

You will need to install the `hexbin` package.
```
install.packages("hexbin")
```

```{r}
pop_hi %>%
  filter(wages > 10000, wages < 300000) %>%
  ggplot(aes(age, wages)) +
  geom_hex() + 
  scale_x_log10()
```

Here we can see that inequality in wages for workers increases with age.

## Other topics from this dataset

This dataset includes information on the majors for degree holders and the industry codes. You could use that
additional information to ask how well targeted majors are to particular industries and how incomes vary
across choice of major. Because of the size of the data in the Hawaii sample, it would be better to ask some
of these questions at the national level.

Go to the [ACS PUMS documentation page](https://www.census.gov/programs-surveys/acs/technical-documentation/pums/documentation.2015.html) for more information.


## Assignment

Choose another pair of variables from the data dictionary and visualize them with a scatterplot,
a heatmap, and a hexbin plot.

<!--chapter:end:02-read-data.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---
```{r include=FALSE, cache=FALSE}
rm(list = ls(all = TRUE))
knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, cache = TRUE)
```

# Facets, Bubbles, and Transparency {#facets-and-bubbles}

## Data

For this session, we'll explore the Hawaii Tourism Authority (HTA) [Air Seat Projection](http://www.hawaiitourismauthority.org/research/research/infrastructure-research/).
I'll be working with the 
[Air Seat Projection for 2017 (revised 06/17)](http://www.hawaiitourismauthority.org/default/assets/File/2017%20Air%20Seat%20Forecast%20rev%200617.xls). Feel free to download the latest available.

### Importing non-standard Excel files

The first steps in preparing a non-standard Excel file are (1) identify how many rows to skip
and (2) provide column names if the column names are not neatly contained in a single row. You may
also want to set the `range` if there is metadata at the end of the table you are importing. `range`
overrides any `skip` setting, so we wont have to specify the number of rows to skip.

```{r}
library(readxl)
seats <- read_excel("data/2017 Air Seat Forecast rev 0617.xls", col_names = c(
  "dep_city", 
  "seats2017Q1", "seats2017Q2", "seats2017Q3", "seats2017Q4", "seats2017", 
  "seats2016Q1", "seats2016Q2", "seats2016Q3", "seats2016Q4", "seats2016",
  "seatschangeQ1", "seatschangeQ2", "seatschangeQ3", "seatschangeQ4", "seatschange"
), range = "A5:P78")
seats
```

Let's add a region identifier
```{r}
library(dplyr)
us_west_range <- 10:23
us_east_range <- 26:33
japan_range <- 40:45
canada_range <- 48:52
other_asia_range <-55:58
oceania_range <- 61:64
other_range <- 67:74

seats$region <- NA
seats[us_west_range,]$region <- "US West"
seats[us_east_range,]$region <- "US East"
seats[japan_range,]$region <- "Japan"
seats[canada_range,]$region <- "Canada"
seats[other_asia_range,]$region <- "Other Asia"
seats[oceania_range,]$region <- "Oceania"
seats[other_range,]$region <- "Other"

seats <- seats %>%
  filter(!is.na(region))
seats
```

## Facets

Let's do a simple plot comparing 2017 seats outlook to the 2016 seats outlook.

```{r}
library(ggplot2)
seats %>%
  ggplot(aes(seats2016, seats2017)) +
  geom_point()
```


The distribution of this data looks like a good candidate for using the log scale (high concentration in lower
values and lower concentration in higher values).

```{r}
seats %>%
  ggplot(aes(seats2016, seats2017)) +
  geom_point() +
  scale_x_log10() +
  scale_y_log10() + 
  geom_abline(lty = 2) # dashed line type (lty)
```

Since we have region identifiers it would be nice to divide our data and see charts of each region side-by-side.
Facets allow us to make multiple charts based on a variable or set of variables.

```{r}
seats %>%
  ggplot(aes(seats2016, seats2017)) +
  geom_point() +
  scale_x_log10() +
  scale_y_log10() + 
  geom_abline(lty = 2) +
  facet_wrap(~ region) +
  coord_fixed()
```


An alternative representation is to present each region using color:
```{r}
seats %>%
  ggplot(aes(seats2016, seats2017, color = region, label = dep_city)) +
  geom_point() +
  scale_x_log10() +
  scale_y_log10() + 
  geom_abline(lty = 2) +
  geom_text(check_overlap = TRUE, nudge_y = 0.1)
```


## Bubbles

Bubble charts are scatter plots (geom_point) with points that vary in size corresponding to the value of
a given variable. Let's create a measure of the size of a city's seats relative to its regional total.

```{r}
seats <- seats %>%
  group_by(region) %>%
  mutate(proportion_of_region = seats2017/sum(seats2017))
seats
```

Now we can modify the chart to show the importance of each city in the context of its region.

```{r}
seats %>%
  filter(region %in% c("US West", "US East")) %>%
  ggplot(aes(seats2016, seats2017, color = region, label = dep_city)) +
  geom_abline(lty = 2) +
  geom_point(aes(size = proportion_of_region)) +
  scale_x_log10(labels = scales::comma) +
  scale_y_log10(labels = scales::comma) + 
  geom_text(check_overlap = TRUE, nudge_y = 0.1)
```

## Transparency

We can also use transparency (or alpha) to make less important points less visible. We do this
by setting the `alpha` aesthetic. Let's try adding the alpha setting to the `geom_point()` call
first.

```{r}
seats %>%
  filter(region %in% c("US West", "US East")) %>%
  ggplot(aes(seats2016, seats2017, color = region, label = dep_city)) +
  geom_abline(lty = 2) +
  geom_point(aes(size = proportion_of_region, alpha = proportion_of_region)) +
  scale_x_log10(labels = scales::comma) +
  scale_y_log10(labels = scales::comma) + 
  geom_text(check_overlap = TRUE, nudge_y = 0.1)
```

Let's add the alpha to the ggplot-level aesthetic instead, so that it also affects the text labels.

```{r}
seats %>%
  filter(region %in% c("US West", "US East")) %>%
  ggplot(aes(seats2016, seats2017, color = region, label = dep_city, alpha = proportion_of_region)) +
  geom_abline(lty = 2) +
  geom_point(aes(size = proportion_of_region)) +
  scale_x_log10(labels = scales::comma) +
  scale_y_log10(labels = scales::comma) + 
  geom_text(nudge_y = 0.1)
```

We can combine all the regions now and use transparency to help us see how many cities are
in the same area on the plot by how dark a region is.

```{r}
seats %>%
  ggplot(aes(seats2016, seats2017, label = dep_city, alpha = proportion_of_region)) +
  geom_abline(lty = 2) +
  geom_point(aes(size = proportion_of_region)) +
  scale_x_log10(labels = scales::comma) +
  scale_y_log10(labels = scales::comma) + 
  geom_text(aes(color = region), hjust = "right", vjust = "center")
```


Let's use facets so we can combine everything we've done so far.


```{r}
seats %>%
  ggplot(aes(seats2016, seats2017, label = dep_city, alpha = proportion_of_region)) +
  geom_abline(lty = 2) +
  geom_point(aes(size = proportion_of_region), color = "darkblue") +
  scale_x_log10(labels = scales::comma) +
  scale_y_log10(labels = scales::comma) + 
  geom_text(hjust = "right", vjust = "center", nudge_x = -0.3) +
  facet_wrap(~ region)
```

## Assignment

Create a bubble plot highlighting the change in year-on-year growth rates for different quarters.
Plot `seatschangeQ3` on the x axis and `seatschangeQ4` on the y axis. Use `seats2017` to determine the
size of each bubble. Facet by region.

<!--chapter:end:03-facets-and-bubbles.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---
```{r include=FALSE, cache=FALSE}
rm(list = ls(all = TRUE))
knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, cache = TRUE)
```

# Lines and Curves

## Data

We will use data on the number of active duty personnel in Hawaii. 
The first dataset is an Excel file pulled from the State of Hawaii Department of Business, Economic Development, and
Tourism (DBEDT) [2015 State of Hawaii Data Book](http://dbedt.hawaii.gov/economic/databook/2015-individual/). See the line listed
as, "10.03 - Active Duty Personnel, by Service: 1953 to 2015."
The data is originally from the [US Defense Manpower Data Center](www.dmdc.osd.mil/appj/dwp/stats_reports.jsp)

```{r}
library(readxl)
library(dplyr)
library(magrittr)
mil_personnel <- read_excel("data/100315.xls", range = "A5:L38", col_types = "numeric")
mil_personnel <- bind_rows(
  select(mil_personnel, 1:6) %>% set_colnames(c("Year", "Total", "Army", "Navy", "Marine Corps", "Air Force")),
  select(mil_personnel, 7:12) %>% set_colnames(c("Year", "Total", "Army", "Navy", "Marine Corps", "Air Force"))
) %>%
  filter(!is.na(Year))
mil_personnel
```


## geom_smooth

`geom_smooth` allows you to have smooth lines appear in your chart. With no argument, it will
choose `loess` for series shorter than 1,000 observations. It shows a shaded confidence interval.

```{r}
library(ggplot2)
mil_personnel %>%
  ggplot(aes(Year, Total)) +
  geom_point() +
  geom_smooth()
```

Here's what it looks like if we fit a linear model instead:

```{r}
mil_personnel %>%
  ggplot(aes(Year, Total)) +
  geom_point() +
  geom_smooth(method = "lm")
```

We can also just have a line chart that connects the points:

```{r}
mil_personnel %>%
  ggplot(aes(Year, Total)) +
  geom_point() +
  geom_line()
```

## geom_abline

`geom_abline` allows you to display lines with a specific intercept and slope. If no intercept or slope is
provided, a 45-degree line will be shown.

```{r}
x = rnorm(100)
y = 2.5 + 1.2 * x + rnorm(100)
test_data <- data_frame(x, y)

test_data %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  xlim(-2, 6) + ylim(-2, 6) +
  coord_fixed() +
  geom_abline() 
```

```{r}
test_data %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  xlim(-2, 6) + ylim(-2, 6) +
  coord_fixed() +
  geom_abline() +
  geom_abline(intercept = 2.5, slope = 1.2, color = "red") 
```

## geom_vline

`geom_vline` allows you to draw vertical lines by specifying an x intercept.

```{r}
test_data %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  xlim(-2, 6) + ylim(-2, 6) +
  coord_fixed() +
  geom_abline() +
  geom_abline(intercept = 2.5, slope = 1.2, color = "red") +
  geom_vline(xintercept = 2, color = "blue")
```

## hline

`geom_vline` allows you to draw vertical lines by specifying an x intercept.

```{r}
test_data %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  xlim(-2, 6) + ylim(-2, 6) +
  coord_fixed() +
  geom_abline() +
  geom_abline(intercept = 2.5, slope = 1.2, color = "red") +
  geom_vline(xintercept = 2, color = "blue") +
  geom_hline(yintercept = 1, color = "#4FCC53", lty = 2)
```

<!--chapter:end:04-lines.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---
```{r include=FALSE, cache=FALSE}
rm(list = ls(all = TRUE))
knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, cache = TRUE)
```

# ggplot2 Extensions {#ggplot-exts}

## Data 

For this section, we'll look at data from the American Community Survey (ACS)
on immigration. To download the data,

1. Go to the [American FactFinder website](https://factfinder.census.gov/).
2. Click on the "Download Center" section, then click the "DOWNLOAD CENTER" button.
3. Click the "NEXT" button, since we know the table we want to download.
4. Select "American Community Survey" from the Program dropdown.
5. Select "2015 ACS 5-year estimates", click the "ADD TO YOUR SELECTIONS" button, then click "NEXT"
6. Select "County - 050" from the geographic type dropdown, then select "All Counties within United States", click the "ADD TO YOUR SELECTIONS" button, then click "NEXT"
7. Type `income mobility` in the "topic or table name"" search box, then select the option that reads: 

"B07011: MEDIAN INCOME IN THE PAST 12 MONTHS (IN 2015 INFLATION-ADJUSTED DOLLARS) BY GEOGRAPHICAL MOBILITY IN THE PAST YEAR FOR CURRENT RESIDENCE IN THE UNITED STATES"

Click "GO", then check the checkbox beside the table we found. Now click on the "Download" button, and 
uncheck the option that says, "Include descriptive data element names." Click "Ok" to create your zip file. Once the file has been created, click "DOWNLOAD" to download the zip file.

The following will assume you moved the following files within the zip to your `data` folder:

* ACS_15_5YR_B07011_with_ann.csv
* ACS_15_5YR_B07011_metadata.csv

The file `ACS_15_5YR_B07011.txt` tells us how to interpret codes within our data. It is possible for median values to be followed by a `+` or `-` if they
are in the upper or lower open-ended interval. In our dataset we don't have any medians in the upper open-ended interval, but we do have entries in the
lower open-ended interval.

```{r, message=FALSE}
library(readr)
acs <- read_csv("data/ACS_15_5YR_B07011_with_ann.csv", col_types = strrep("c", 15), na = c("-", "(X)"))
meta <- read_csv("data/ACS_15_5YR_B07011_metadata.csv")
meta
```


Let's keep only the variables we care about, using more informative variable names.
```{r}
library(dplyr)
acs_mobility <- acs %>%
  transmute(
    geo = `GEO.display-label`,
    same_house = HD01_VD03,
    same_county = HD01_VD04,
    same_state = HD01_VD05,
    same_country = HD01_VD06,
    different_country = HD01_VD07
  )
acs_mobility
```

Now we can add an indicator for whether the median value is in the lowest available interval. This would mean that the median value presented has been bottom-coded.
```{r}
acs_mobility <- acs_mobility %>%
  mutate(
    same_country_bc = grepl("[0-9]*-", same_country),
    different_country_bc = grepl("[0-9]*-", different_country)
  )
acs_mobility
```

Let's see how many counties have observations that are bottom coded:
```{r}
acs_mobility %>%
  summarize(same_country_bc = sum(same_country_bc), different_country_bc = sum(different_country_bc), counties = n())
```

Let's see what the typical bottom-coded values are:
```{r}
acs_mobility %>%
  filter(same_country_bc) %>%
  select(same_country) %>%
  table()
```

```{r}
acs_mobility %>%
  filter(different_country_bc) %>%
  select(different_country) %>%
  table()
```

In both cases the bottom-coded interval is the range from zero to 2,500. Since this is a small number of counties given the entire range, let's simply
set the bottom-coded values to equal the upper-bound of their interval (i.e., 2,500).
```{r}
acs_mobility <- acs_mobility %>%
  transmute(
    geo = geo,
    same_house = as.integer(same_house),
    same_county = as.integer(same_county),
    same_state = as.integer(same_state),
    same_country = if_else(same_country_bc, 2500L, as.integer(same_country)),
    different_country = if_else(different_country_bc, 2500L, as.integer(different_country))
  )
acs_mobility
```

Let's rearrange the data into `tidy` format (one observation per row).
```{r}
library(tidyr)
tidy_acs <- acs_mobility %>%
  gather(location_last_year, median_income, -geo, factor_key = TRUE)
tidy_acs
```

## ggplot2 extensions

There are many extensions the community have made that build on ggplot2. 
The following link provides a gallery of many of these extensions: 

[ggplot2 extensions](http://www.ggplot2-exts.org/gallery)

Some others that are usefull are `ggjoy` and `GGally`.

## ggjoy

Make sure `ggjoy` is installed.

```
install.packages("ggjoy")
```

`ggjoy` gives us the ability to stack kernel density plots.

```{r}
library(ggjoy)
ggplot(tidy_acs, aes(x = median_income, y = location_last_year, group(location_last_year))) +
  geom_joy() +
  theme_joy()
```

This plot shows us that, on average, the distance moved in the past year is inversely related to median income.

## scatterplot matrix (GGally::ggscatmat)

Make sure you have `GGally` installed.

```
install.packages("GGally")
```

One particular library, [GGally](http://ggobi.github.io/ggally/), has a great set of visualizations
to extend those that come prebuilt with `ggplot`. One common visualization tool that is missing from
ggplot is the scatterplot matrix. While base R provides `splom()` in the `lattice` library, 
`GGally::ggpairs` and `GGally::ggscatmat` pr ovide an easy tool to create a scatterplot matrix with
ggplot2.

```{r}
library(GGally)
acs_mobility %>%
  as.data.frame() %>%
  ggscatmat(columns = 2:ncol(.), alpha = 0.1)
```



<!--chapter:end:05-ggplot-exts.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---
```{r include=FALSE, cache=FALSE}
rm(list = ls(all = TRUE))
knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, cache = TRUE)
```

# Boxplots and Violin Plots {#boxplots-and-violins}

Boxplots and violin plots are two important tools for visualizing the distribution of
data within a dataset. The boxplot highlights the median, key percentiles, and outliers
within a dataset. The violin plot takes a kernel density plot, rotates it 90 degrees, then
mirrors it about the axis to create a shape that sometimes resembles a violin.

## Data 

The Social Security Administration releases data on earnings and employment
each year. We'll take a look at the data for 2014:

https://www.ssa.gov/policy/docs/statcomps/eedata_sc/2014/index.html

We're going to download Table 1: "Number of persons with Social Security 
(OASDI) taxable earnings, amount taxable, and contributions, by state or 
other area, sex, and type of earnings, 2014"

Save that file as 'ssa_earnings.xlsx' in the `data` folder

```{r}
library(readxl)
ssa <- read_xlsx("data/ssa_earnings.xlsx", range = "A7:J159", 
                 col_names = c("state", "gender", "other", "other2", "number.total", "number.wage", "number.self", 
                               "earnings.total", "earnings.wage", "earnings.self"))
ssa
```

The starting format is far from ideal. Each row should represent one group,
so we don't need any of the rows with totals.

It's important to always read any footnotes and documentation that comes with
the data you plan to use. Footnote `c` for this table indicates that individuals
with both wage and salary employment will be counted in both groups, but only
once in the total. It is important to be aware of this double counting.

```{r}
library(tidyr)
library(dplyr)
ssa_long <- ssa %>%
  fill(state) %>%
  filter(!is.na(gender)) %>%
  reshape(varying = 5:10, direction = "long", timevar = "earnings_type") %>%
  select(state, gender, earnings_type, number, earnings) %>%
  mutate(per_capita = earnings / number)
```

## Boxplots

```{r}
library(ggplot2)
ssa_long %>%
  filter(earnings_type != "total") %>%
  ggplot(aes(gender, per_capita)) +
  geom_boxplot()
```

```{r}
ssa_long %>%
  ggplot(aes(gender, per_capita, fill = gender)) +
  geom_boxplot() +
  facet_grid(~ earnings_type)
```


## Violin Plots

Let's repeat the above plots using the violin plot type.

```{r}
ssa_long %>%
  filter(earnings_type != "total") %>%
  ggplot(aes(gender, per_capita)) +
  geom_violin()
```


```{r}
ssa_long %>%
  ggplot(aes(gender, per_capita, color = gender, fill = gender)) +
  geom_violin() +
  facet_grid(~ earnings_type)
```


## Dot Plots

Dot plots appear similar to violin plots, but dot plots may be easier to interpret:

```{r}
ssa_long %>%
  ggplot(aes(gender, per_capita, color = gender, fill = gender)) +
  geom_dotplot(binaxis = "y", stackdir = "center", position = "dodge") +
  facet_grid(~ earnings_type)
```

## Assignment

Create your own visualization of the SSA data using one of the above distribution visualizations.


<!--chapter:end:06-boxplots-and-violins.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---
```{r include=FALSE, cache=FALSE}
rm(list = ls(all = TRUE))
knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, cache = TRUE)
```

# Spatial Visualizations {#geom_spoke}

## Data

The data for this class will come from the National Oceanic and Atmospheric Administration (NOAA) U.S. Wind Climatology
datasets (https://www.ncdc.noaa.gov/societal-impacts/wind/).

Download the files for both the u-component and the v-component of the wind data. To open these files in R, we'll need
to install the ncdf4 package, which provides an interface to Unidata's netCDF data file format:

```
install.packages(c("ncdf4", "ncdf4.helpers", "PCICt"))
```

Let's load up the u-component file first:

```{r}
library(ncdf4)

uwnd_nc <- nc_open("data/uwnd.sig995.2017.nc")
uwnd_nc
```

Let's store the uwnd observations in the netCDF file for the u-component:
```{r}
## packages for getting nice time variables from the netCDF file
library(ncdf4.helpers)
library(PCICt)
## general data munging packages
library(dplyr)
library(tibble)

uwnd <- ncvar_get(uwnd_nc, "uwnd")
uwnd_time <- nc.get.time.series(uwnd_nc, v = "uwnd", time.dim.name = "time")
uwnd_lon <- ncvar_get(uwnd_nc, "lon")
uwnd_lat <- ncvar_get(uwnd_nc, "lat")
nc_close(uwnd_nc)

uwnd_df <- uwnd %>%
  as.data.frame.table(responseName = "uwnd", stringsAsFactors = FALSE) %>%
  rename(a = Var1, b = Var2, c = Var3) %>%
  cbind.data.frame(expand.grid(uwnd_lon, uwnd_lat, uwnd_time)) %>%
  rename(lon = Var1, lat = Var2, time = Var3) %>%
  select(lon, lat, time, uwnd) %>%
  as.tibble()
uwnd_df
```

Now we need to do the same for the v-component of the wind vectors. Since we know the lat, lon, and time dimensions are repeated, 
we can join directly to the previous data.frame:
```{r}
vwnd_nc <- nc_open("data/vwnd.sig995.2017.nc")
vwnd <- ncvar_get(vwnd_nc, "vwnd")
vwnd_time <- nc.get.time.series(vwnd_nc, v = "vwnd", time.dim.name = "time")
vwnd_lon <- ncvar_get(vwnd_nc, "lon")
vwnd_lat <- ncvar_get(vwnd_nc, "lat")
nc_close(vwnd_nc)

wind <- vwnd %>%
  as.data.frame.table(responseName = "vwnd", stringsAsFactors = FALSE) %>%
  cbind.data.frame(uwnd_df) %>%
  rename(lon2 = Var1, lat2 = Var2, time2 = Var3) %>%
  select(lon, lat, time, vwnd, uwnd) %>%
  as.tibble()
wind
```

Otherwise, we would need to merge these data.frames to get `uwnd` and `vwnd` together with the following, which takes long time to run:
```
wind <- merge(uwnd_df, vwnd_df)
```


## geom_spoke

To represent these wind vectors we'll use the `geom_spoke()`. We'll start just plotting wind
patterns for January 1, 2017:
```{r}
library(ggplot2)
wind <- wind %>%
  mutate(angle = atan2(vwnd, uwnd), radius = sqrt(uwnd^2 + vwnd^2), time = as.POSIXct(time))
wind %>%
  filter(time == as.POSIXct("2017-01-01", tz = "GMT")) %>%
  ggplot(aes(lon, lat)) +
  geom_spoke(aes(angle = angle, radius = radius, alpha = radius, color = angle)) +
  scale_color_gradient2(low = "#132B43", mid = "#56B1F7", high = "#132B43")
```


## maps

```
install.packages("maps")
```

Map data will help to provide some context to this wind figure. We'll use `geom_polygon` to
plot the world centered on the Pacific Ocean (`world2`) using the `map_data()` function.


```{r}
world <- map_data("world2")
wind %>%
  filter(time == as.POSIXct("2017-01-01", tz = "GMT")) %>%
  ggplot(aes(lon, lat)) +
  geom_polygon(data = world, aes(x=long, y = lat, group = group), color = "green", fill = NA) + 
  coord_fixed(1) +
  geom_spoke(aes(angle = angle, radius = radius, alpha = radius, color = angle)) +
  scale_color_gradient2(low = "#132B43", mid = "#56B1F7", high = "#132B43") +
  theme_minimal()
```


## gganimate

The `gganimate` package lets us animate the above chart.
If you want to be able to save animations as an mp4, you will need install `ffmpeg` (https://www.ffmpeg.org/download.html). You can install `gganimate` with `devtools`:
```
devtools::install_github("dgrtwo/gganimate")
```

```{r wind-animation, fig.show="animate", interval=0.1, cache=TRUE}
library(gganimate)
f <- wind %>%
  ggplot(aes(lon, lat)) +
  geom_polygon(data = world, aes(x=long, y = lat, group = group), color = "green", fill = NA) + 
  coord_fixed(1) +
  geom_spoke(aes(angle = angle, radius = radius, alpha = radius, color = angle, frame = time)) +
  scale_color_gradient2(low = "#132B43", mid = "#56B1F7", high = "#132B43") +
  theme_minimal()
gganimate(f)
```

## glyphs

`glyphs` provide another useful way of analyzing spatial data with a time dimesion. This shows
a tiny line charts representing the north-south component of the wind at each longitude/latitude
combination.

```{r}
library(GGally)
wind$day <- as.numeric(julian(wind$time, as.POSIXct("2017-01-01", tz = "GMT")))
wind$day_flip <- -wind$day
vwnd_gly <- glyphs(wind, "lon", "day", "lat", "vwnd", height=2.5)
uwnd_gly <- glyphs(wind, "lon", "day", "lat", "uwnd", height=2.5)

ggplot(vwnd_gly, aes(gx, gy, group = gid)) +
  add_ref_lines(vwnd_gly, color = "grey90") +
  add_ref_boxes(vwnd_gly, color = "grey90") +
  geom_path() +
  theme_bw() +
  labs(x = "", y = "")
```

Let's focus in on just the continental US:

```{r, fig.show="hold"}
usa <- map_data("usa")
usa_long_range <- range(usa$long)
usa_lat_range <- range(usa$lat)

usa_wind <- wind %>%
  filter(lon >= (usa_long_range[1] %% 360) & lon <= (usa_long_range[2] %% 360) &
           lat >= usa_lat_range[1] & lat <= usa_lat_range[2])
usa_wind$day <- as.numeric(julian(usa_wind$time, as.POSIXct("2017-01-01", tz = "GMT")))
usa_wind$day_flip <- -usa_wind$day
usa_vwnd_gly <- glyphs(usa_wind, "lon", "day", "lat", "vwnd", height=2.5)
usa_uwnd_gly <- glyphs(usa_wind, "lon", "uwnd", "lat", "day_flip", height=2.5)

ggplot(usa_vwnd_gly, aes(gx, gy, group = gid)) +
  geom_polygon(data = usa, aes(x = long %% 360, y = lat %% 360, group = group), fill = "grey60") +
  add_ref_lines(usa_vwnd_gly, color = "grey90") +
  add_ref_boxes(usa_vwnd_gly, color = "grey90") +
  geom_path(alpha = 0.9) +
  theme_bw() +
  labs(x = "", y = "", title = "North-South")
ggplot(usa_uwnd_gly, aes(gx, gy, group = gid)) +
  geom_polygon(data = usa, aes(x = long %% 360, y = lat %% 360, group = group), fill = "grey60") +
  add_ref_lines(usa_uwnd_gly, color = "grey90") +
  add_ref_boxes(usa_uwnd_gly, color = "grey90") +
  geom_path(alpha = 0.9) +
  theme_bw() +
  labs(x = "", y = "", title = "East-West")
```


<!--chapter:end:07-geom_spoke.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---
```{r include=FALSE, cache=FALSE}
rm(list = ls(all = TRUE))
knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, cache = TRUE)
```

# geom_area and geom_ribbon {#area-and-ribbons}

## Data 

The US Bureau of Labor Statistics (BLS) conducts the American Time Use Survey (ATUS). You can download
the text form of the ATUS by going to the [BLS data page](https://www.bls.gov/data/), finding the
section labelled Spending & Time Use, then clicking on the "Text Files" button on the row for
the ATUS. Or by using the following link:

https://download.bls.gov/pub/time.series/tu/

### Downloading a file from the internet

While you can manually download the files from the above URL, 
`download.file()` lets you download files from within R.
The first argument is the URL of the resource you want to download. 
The second argument is the destination for the file. The following requests will
require you to create the `tu` folder.

```
download.file("https://download.bls.gov/pub/time.series/tu/tu.txt", "data/tu/tu.txt")
download.file("https://download.bls.gov/pub/time.series/tu/tu.series", "data/tu/tu.series")
download.file("https://download.bls.gov/pub/time.series/tu/tu.data.0.Current", "data/tu/tu.data.0.Current")
```
The file `tu.txt` contains the documentation for the time use (tu) survey data. Section 2 of 
that file provides descriptions of each of the files in the `pub/time.series/tu` folder. From that 
list we can see that `tu.series` will give us a list of the available series.


```{r}
library(readr)
series_defn <- read_tsv("data/tu/tu.series")
series_defn
```

There is a lot here to process. The columns we care most about for now are `series_id` and `series_title`. Using `select()` from the `dplyr` library, we can show just the columns we
care about.

```{r}
library(dplyr)
series_defn %>%
  select(series_id, series_title)
```

### Pairing down the list of variables

Let's look for variables on sleep, work, and leisure:

```{r}
series_defn %>%
  select(series_title) %>%
  filter(grepl("sleep", series_title, ignore.case = TRUE))
```

Since this simple search returns a ton of results, let's further filter by 'employed' and 'per day':
```{r}
series_defn %>%
  select(series_title) %>%
  filter(grepl("per day.*sleep.*employed", series_title, ignore.case = TRUE))
```

Now let's filter further by 'employed full time', 'nonholiday weekdays', and 'on days worked':

```{r}
series_defn %>%
  select(series_title) %>%
  filter(grepl("per day.*sleep.*nonholiday weekdays.*employed full time.*on days worked", series_title, ignore.case = TRUE))
```

Finally, let's filter that to exclude the 'participants only' group and only get the Men/Women
values (not the combined totals):

```{r}
series_defn %>%
  select(series_title) %>%
  filter(grepl("per day -.*sleep.*nonholiday weekdays.*employed full time.*on days worked,", series_title, ignore.case = TRUE))
```

### Adding more activity categories

Now let's add 'work' and 'leisure' to our search:

```{r}
activity <- series_defn %>%
  select(series_id, series_title) %>%
  filter(grepl("per day -.*(sleep|work|leisure).*nonholiday weekdays.*employed full time.*on days worked,", series_title, ignore.case = TRUE))
activity
```

Now we should create a variable that codes each of these as either work, sleep, or leisure:

```{r}
activity <- activity %>%
  mutate(
    activity_type = case_when(
      grepl("leisure", activity$series_title, ignore.case = TRUE) ~ "Leisure",
      grepl("sleep", activity$series_title, ignore.case = TRUE) ~ "Sleep",
      TRUE ~ "Work"
    ), 
    sex = ifelse(grepl("Men", series_title), "Men", "Women")
  )
activity
```

Now we can join the activity data.frame with the current data and create time series of
each activity type we created.
```{r}
data <- read_tsv("data/tu/tu.data.0.Current")
data <- data %>%
  inner_join(activity) %>%
  group_by(year, sex, activity_type) %>%
  summarize(hours = sum(as.numeric(value), na.rm = TRUE))
data
```


## geom_area

`geom_area` is useful when components that naturally add to each other:

```{r}
library(ggplot2)
ggplot(data, aes(year, hours, fill= activity_type)) + geom_area() + facet_wrap(~ sex) 
```

## geom_ribbon

```{r}
data %>%
  ggplot(aes(x = year, group = sex, fill = activity_type)) + 
  geom_ribbon(mapping = aes(ymin = -hours * (sex == "Women"), ymax = hours * (sex == "Men")), data = . %>% filter(activity_type == "Work"), alpha = 0.5) +
  geom_ribbon(mapping = aes(ymin = -hours * (sex == "Women"), ymax = hours * (sex == "Men")), data = . %>% filter(activity_type == "Leisure"), alpha = 0.5) +
  geom_ribbon(mapping = aes(ymin = -hours * (sex == "Women"), ymax = hours * (sex == "Men")), data = . %>% filter(activity_type == "Sleep"), alpha = 0.5) +
  scale_y_continuous(
    name = "Average hours per work day (Fully Employed)",
    breaks = c(-20, -10, 0, 10, 20),
    labels = c("Women 20 hrs", "10 hrs", "0 hrs", "10 hrs", "Men 20 hrs"),
    limits = c(-20, 20)
    )
```

<!--chapter:end:08-area-and-ribbons.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---
```{r include=FALSE, cache=FALSE}
rm(list = ls(all = TRUE))
knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, cache = TRUE)
```

# Jitter, Rug, and Aesthetics {#jitter-rug}

## Data



## Jitter

## Rug

## Aesthetics

<!--chapter:end:09-jitter-rug.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---
```{r include=FALSE, cache=FALSE}
rm(list = ls(all = TRUE))
knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, cache = TRUE)
```

# Themes, Labels, and Colors {#themes-labels-colors}

## Data

The [Panel Study of Income Dynamics (PSID)](https://psidonline.isr.umich.edu) is the 
longest running longitudinal household survey in the world.

From the [Data page](https://simba.isr.umich.edu/data/data.aspx), you can use the `Data Center`
to create customized datasets. We'll use the 
[Packaged Data](https://simba.isr.umich.edu/data/PackagedData.aspx) option. Click the 
[Main and Supplemental Studies](https://simba.isr.umich.edu/Zips/ZipMain.aspx) link. Under 
the `Supplemental Studies > Transition into Adulthood Supplement` section, select the download
for `2015`.

To download the supplement you will need to sign in or register for a new account (by clicking the
"New User?" link). Once you have logged in you should be able to download the zip file:

* ta2015.zip

The `TA2015_codebook.pdf` is the perfect place for us to identify some key variables of interest.
The following is an excerpt listing the variables we will use:

```
TA150003  "2015 PSID FAMILY IW (ID) NUMBER"
2015 PSID Family Interview Number

TA150004  "2015 INDIVIDUAL SEQUENCE NUMBER"
2015 PSID Sequence Number
This variable provides a means of identifying an individual's status with regard to the
PSID family unit at the time of the 2015 PSID interview.

Value/Range   Code Value/Range Text
1 - 20        Individuals in the family at the time of the 2015 PSID
              interview
51 - 59       Individuals in institutions at the time of the 2015 PSID
              interview

TA150005 "CURRENT STATE"
Current State (FIPS state codes)

TA150015 "A1_1 HOW SATISFIED W/ LIFE AS A WHOLE"
A1_1. We'd like to start by asking you about life in general. Please think about your
life-as-a-whole. How satisfied are you with it? Are you completely satisfied, very
satisfied, somewhat satisfied, not very satisfied, or not at all satisfied?

Value/Range   Code Value/Range Text
1             Completely satisfied
2             Very satisfied
3             Somewhat satisfied
4             Not very satisfied
5             Not at all satisfied
8             DK

TA150092 "D28A NUMBER OF CHILDREN"
D28a. How many (biological,) adopted, or step-children do you have?

TA150128 "E1 EMPLOYMENT STATUS 1ST MENTION"
E1. Now we have some questions about employment. We would like to know about what you do -
- are you working now, looking for work, keeping house, a student, or what?--1ST MENTION

Value/Range   Code Value/Range Text
1             Working now, including military
2             Only temporarily laid off; sick or maternity leave
3             Looking for work, unemployed
5             Disabled, permanently or temporarily
6             Keeping house
7             Student

TA150512 "F1 HOW MUCH EARN LAST YEAR"
F1. We try to understand how people all over the country are getting along financially, so
now I have some questions about earnings and income. How much did you earn altogether
from work in 2014, that is, before anything was deducted for taxes or other things,
including any income from bonuses, overtime, tips, commissions, military pay or any other
source?

Value/Range     Code Value/Range Text
0 - 5,000,000   Actual amount
    9,999,998   DK
    9,999,999   NA; refused
```

In preparation for working with these variables, we can setup functions to take the place
of the codebook. The `tigris` package will give us the FIPS codes for each state:
```
install.packages(tigris)
```
```{r}
library(tidyverse)

state_fips <- tigris::fips_codes %>%
  group_by(state) %>%
  summarize(fips = as.numeric(first(state_code)))
fips2state <- array()
fips2state[state_fips$fips] <- state_fips$state

satisfaction <- array()
satisfaction[c(1, 2, 3, 4, 5, 8)] <- c("Completely satisfied", "Very satisfied", "Somewhat satisfied", "Not very satisfied", "Not at all satisfied", "DK")
```


If you don't have them already, open the zip file and move the `TA2015.txt` and `TA2015.sps` files 
into the `data` folder. For our import to work, We need to find a line that needs to be removed 
from the top of the sps file. The line we want to remove should look like the following line:

```
FILE HANDLE PSID / NAME = '[PATH]\TA2015.TXT' LRECL = 2173 .
```

To find this line we can output the first 20 lines of the `TA2015.sps` file:
```{r}
readLines("data/TA2015.sps", n = 10)
```

Now we know the line to remove is line number 9, we can write a new file to be
used in the processing step below.

```{r}
input <- file("data/TA2015.sps")
output <- file("data/TA2015_clean.sps")

open(input, type = "r")
open(output, open = "w")

writeLines(readLines(input, n = 8), output)
invisible(readLines(input, n = 1))
writeLines(readLines(input), output)

close(input)
close(output)

readLines("data/TA2015_clean.sps", n = 10)
```

### memisc

The `memisc` package has useful tools for importing SPSS and Stata files that augment what
already exists in `base`.

```
install.packages("memisc")
```

```{r}
library(memisc)

ta_importer <-spss.fixed.file("data/TA2015.txt", columns.file = "data/TA2015_clean.sps", varlab.file = "data/TA2015_clean.sps", to.lower = FALSE)
ta <- as.data.set(ta_importer) %>%
  as.data.frame() %>%
  filter(TA150005 > 0) %>% # get rid of the 1 non-US response
  transmute(
    family_id         = TA150003, 
    in_institution    = TA150004 > 50, 
    state             = fips2state[TA150005], 
    life_satisfaction = satisfaction[TA150015], 
    children          = TA150092, 
    employment_status = TA150128, 
    annual_earnings   = TA150512
  )
```


## Themes

Themes, ggthemes

## Labels

data labels, titles, and data labels (text)

## Colors


## Assignment

Open the codebook (`data/TA2015_codebook.pdf`) and search for two new variables to visualize and create a
plot that makes use of both jitter and rug.

<!--chapter:end:10-themes-labels-colors.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---
```{r include=FALSE, cache=FALSE}
rm(list = ls(all = TRUE))
knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, cache = TRUE)
```

# (PART) Topics {-} 

# Data Sources Overview {-#data-sources}

While you can find many data sources by typing `public data sources` in your favorite search engine,
the following lists should help you get started.

## Macro Data {-}

### US {-}

* [Federal Reserve Economic Data (FRED)](https://fred.stlouisfed.org/)
* [Bureau of Labor Statistics](https://www.bls.gov/data/)
* [Bureau of Economic Analysis](https://www.bea.gov/data.htm)
* [National Bureau of Economic Research](http://www.nber.org/data/)
* [Congressional Budget Office: Budget and Economic Data](https://www.cbo.gov/about/products/budget-economic-data)
* [American FactFinder (American Community Survey, Census Summary Files, etc.)](https://factfinder.census.gov/)

### Other US {-}

* [The Conference Board](https://www.conference-board.org/data/) (includes consumer confidence index)
* [Survey of Consumers](http://www.sca.isr.umich.edu/tables.html)
* [Historical Exchange Rates](https://measuringworth.com/datasets/)
* [Center for Medicare and Medicaid Services](https://dnav.cms.gov)

## Micro Data {-}
* [Panel Study on Income Dynamics](http://simba.isr.umich.edu/data/data.aspx)
* [IPUMS (census and survey data)](https://www.ipums.org/)
* [US Census Public Use Microdata Sample (PUMS)](https://www.census.gov/programs-surveys/acs/data/pums.html)
* [Center for Medicare and Medicaid Services](https://dnav.cms.gov)

## Hawaii Data {-}

* [State of Hawaii Department of Business, Economic Development and Tourism (DBEDT)](http://dbedt.hawaii.gov/economic/databook/)

## Collections of data lists {-}

* [American Economic Association (AEA) list of data sources](https://www.aeaweb.org/resources/data)



<!--chapter:end:21-data-sources.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---
```{r include=FALSE, cache=FALSE}
rm(list = ls(all = TRUE))
knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, cache = TRUE)
```

# Anscombe's Quartet {-#anscombe}

Anscombe quartet emphasizes the need to move beyond basic numerical summaries of your data.
The `anscombe` dataset has four sets of `x` and `y` variables with very similar summaries, 
but distinct visual patterns

## Prep the data {-}

```{r}
anscombe
```

First we'll use `tidyr` to reshape the anscombe dataset to make it easier to work with.

```{r}
library(tidyr)
tidy_anscombe <- anscombe %>%
  mutate(id = row_number()) %>%
  gather(key = key, value = value, everything(), -id)
tidy_anscombe
```

```{r}
tidy_anscombe <- tidy_anscombe %>%
  separate(key, c("x_or_y", "series"), 1)
tidy_anscombe
```

```{r}
tidy_anscombe <- tidy_anscombe %>%
  spread(x_or_y, value)
tidy_anscombe
```


## Numeric summary {-}

```{r}
tidy_anscombe %>%
  group_by(series) %>%
  summarise(
    mean_x = mean(x),
    mean_y = mean(y),
    sd_x = sd(x),
    sd_y = sd(y),
    cor = cor(x, y)
  )
```



## Visual summary {-}

While the visual summaries suggest very similar datasets, the visual summaries help identify the differences:

```{r}
library(ggplot2)
tidy_anscombe %>%
  ggplot(aes(x, y)) +
  geom_point() +
  facet_wrap(~ series) +
  coord_fixed()
```


## The Datasaurus Dozen {-}

The Datasaurus Dozen is a set of series, like the Anscombe's quartet with similar numerical summaries and
radically different visual summaries. See a great discussion of this dataset by the creators, Justin Matejka and
George Fitzmaurice [here](https://www.autodeskresearch.com/publications/samestats)

Download the data [here](https://www.autodeskresearch.com/sites/default/files/The%20Datasaurus%20Dozen.zip) and 
move the DatasaurusDozen.tsv file into your data folder.

```{r}
datasaurus <- read_tsv("data/DatasaurusDozen.tsv")
datasaurus %>%
  group_by(dataset) %>%
  summarise(
    mean_x = mean(x),
    mean_y = mean(y),
    sd_x = sd(x),
    sd_y = sd(y),
    cor = cor(x, y)
  )
```

Visual summaries
```{r}
datasaurus %>%
  ggplot(aes(x, y)) +
  geom_point() +
  facet_wrap(~ dataset, ncol = 6) +
  coord_fixed()
```

<!--chapter:end:22-anscombe.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---
```{r include=FALSE, cache=FALSE}
rm(list = ls(all = TRUE))
knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, cache = TRUE)
```

# How to Judge Visualizations {-#trifecta}

## Pre-class assignment {-}

Read [Junk Charts Trifecta Checkup](http://junkcharts.typepad.com/junk_charts/junk-charts-trifecta-checkup-the-definitive-guide.html)

## Lecture {-}

[Kaiser Fung](http://junkcharts.typepad.com/numbersruleyourworld/biography.html) has a
useful framework for judging the quality of data visualizations. He calls his framework
the
[Junk Charts Trifecta Checkup](http://junkcharts.typepad.com/junk_charts/junk-charts-trifecta-checkup-the-definitive-guide.html). To make that shorter, we'll just call this
framework the *Trifecta Checkup*. This framework boils down to the following 3 questions:

1. What is the **question**?
2. What does the **data** say?
3. What does the **visual** say?

Each visualization can be labeled according to which question(s) it does not answer 
well. For example, a chart with a meaningful question and relevant data, but ineffective visuals, would be labeled **v**. A chart that gets the data and question wrong, but has a nice visual, would be labeled **qd**.

## Assignment {-}

Assign a label to each of the following visualizations based on the *Trifecta Checkup*.
Provide at least one sentence per question defending your chosen labels.

<!--chapter:end:25-trifecta.Rmd-->

