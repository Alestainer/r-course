[
["index.html", "R for Data Analysis and Visualization Syllabus Office Hours Student Learning Objectives Resources Course Requirements Individual Project Schedule Other Resources", " R for Data Analysis and Visualization ECON 396 (Fall 2017) TR 10:30-11:45, Webster Hall 112 Jonathan Page 2017-06-25 Syllabus Office Hours Monday 2-3 PM and Tuesday 3-4 PM, or by appointment, Saunders 509, jrpage at hawaii dot edu. Student Learning Objectives To be familiar with standard techniques for visualizing data, including heat maps, contour plots, (look at the list in the Data Visualization book) To be able to transform raw data into formats suitable for analysis To be able to perform basic exploratory analysis To be able to create data visualizations There is no prerequisite for this course. Resources Required Introductory Statistics with Randomization and Simulation: Available as a free PDF (https://www.openintro.org/stat/textbook.php?stat_book=isrs) or for $8.49 on Amazon. R Graphics Cookbook Recommended: ggplot2 Cheatsheet Course Requirements Grades for this course will be based on weekly assignments (30%), project assignments (30%), the project proposal (5%), the final project deliverable (15%), and final project presentation participation (20%). Weekly assignments (30%) Weekly assignments are short R excercises. Each exercise should take no longer than 15 minutes. You will typically be given time to complete the exercise in class the day the assignment is given. The assignment will be in the form of R Markdown file (*.Rmd). You will submit the completed assignments via classroom.google.com by the following class period. Individual Project Project assignments (30%) Each week, leading up to the project proposal, you will be given an assignment that is designed to provide you with an organized workflow for approaching new data science projects. Project assignments are submitted via classroom.google.com, with the exception of the two presentations Project proposal presentation (5%) This presentation should be less than 2 minutes. You simply need to communicate the core question your project seeks to answer and the dataset(s) you will be using to answer this question. Final project (15%) The final project will be an R Markdown document which communicates your project question, the data you used, and your results. Final project presentation participation (20%) Your final project participation grade is based on a combination of your own presentation and the feedback you provide to your classmates. Schedule The following schedule is tentative and subject to change. Week 1 R Intro to R and RStudio; Histograms, scatterplots, summary statistics Topic Data sources overview Project Assignment Indentify interesting datasets and questions Week 2 R read_csv, dplyr basics, heatmaps, hexbins Topic Anscombe’s Quartet Project Assignment Choose question and dataset for your project Week 3 R ggplot facets, bubble plots, transparency Topic Effective Data Visualization Project Assignment Write description of your question Week 4 R geom_smooth, abline, vline, hline, ggTimeSeries Topic Time series analysis Project Assignment Write description of your dataset(s) Week 5 R ggplot2 extensions, scatterplot matrix (GGally) Topic JunkCharts Trifecta Checkup Project Assignment Create 2 descriptive plots of your datasets(s) Week 6 R Boxplots, violin plots Topic Project Assignment Write a description of the data cleaning required for your project Week 7 R geom_spoke, maps Topic Project Assignment Write a description of your planned approach Week 8 R geom_area, geom_ribbon Topic Project Proposal Description Project Assignment Work on project proposal presentation Week 9 R jitter, rug, aesthetics Project Assignment Present project proposal (&lt;2 Minutes) Week 10 R themes, ggthemes extension, labels, color scales Topic Project Assignment Work on final project Week 11 R polar coordinates, ggradar extension Topic Project Assignment Work on final project (cont.) Week 12 R word/text analysis Topic Project Assignment Work on final project (cont.) Week 13 R gganimate Topic Project Assignment Work on final project (cont.) Week 14 R git and GitHub for R Topic Project Assignment Week 15 R networks, geomnet extension Topic Final Project presentations Project Assignment Other Resources There are many useful resources you should be aware of while going through this course: Statistics Variance Explained R for Data Science - Grolemund and Wickham Visualization FlowingData Junk Charts Courses Gary King - Quantitative Research Methodology John Stasko - Information Visualization Jenny Bryan - Data wrangling, exploration, and analysis with R Books Econometrics in R Using R for Data Analysis and Graphics Papers Embedded Plots "],
["intro.html", "Lecture 1 R Basics 1.1 R Markdown 1.2 Working with data already loaded into R 1.3 Assignment", " Lecture 1 R Basics Before we begin, make sure you have R and RStudio installed. 1.1 R Markdown Throughout this course, R Markdown will make our lives easier. Make sure that the rmarkdown library is installed: install.packages(&quot;rmarkdown&quot;) For each assignment, you will create an R Markdown file (*.Rmd) and submit that file by the following class session using classroom.google.com 1.2 Working with data already loaded into R Base R comes with a set of sample data that is useful for illustrating techniques in R. Run the following command to see a list of the datasets in the core library datasets: library(help = &quot;datasets&quot;) These datasets are accessible automatically. We’ll start with the Swiss Fertility and Socioeconomic Inicators (1888) dataset. See a description of the dataset by using the help command, either ?swiss or help(swiss). This dataset is technically a data.frame, which you can see by using the command class(swiss). For more information on data.frames take a look at the documentation(help(data.frame)) 1.2.1 Numeric summaries Here are a few ways we can summarize a dataset: head() shows us the first six rows of a data.frame. head(swiss) ## Fertility Agriculture Examination Education Catholic ## Courtelary 80.2 17.0 15 12 9.96 ## Delemont 83.1 45.1 6 9 84.84 ## Franches-Mnt 92.5 39.7 5 5 93.40 ## Moutier 85.8 36.5 12 7 33.77 ## Neuveville 76.9 43.5 17 15 5.16 ## Porrentruy 76.1 35.3 9 7 90.57 ## Infant.Mortality ## Courtelary 22.2 ## Delemont 22.2 ## Franches-Mnt 20.2 ## Moutier 20.3 ## Neuveville 20.6 ## Porrentruy 26.6 summary() provides summary statistics for each column in a data.frame. summary(swiss) ## Fertility Agriculture Examination Education ## Min. :35.00 Min. : 1.20 Min. : 3.00 Min. : 1.00 ## 1st Qu.:64.70 1st Qu.:35.90 1st Qu.:12.00 1st Qu.: 6.00 ## Median :70.40 Median :54.10 Median :16.00 Median : 8.00 ## Mean :70.14 Mean :50.66 Mean :16.49 Mean :10.98 ## 3rd Qu.:78.45 3rd Qu.:67.65 3rd Qu.:22.00 3rd Qu.:12.00 ## Max. :92.50 Max. :89.70 Max. :37.00 Max. :53.00 ## Catholic Infant.Mortality ## Min. : 2.150 Min. :10.80 ## 1st Qu.: 5.195 1st Qu.:18.15 ## Median : 15.140 Median :20.00 ## Mean : 41.144 Mean :19.94 ## 3rd Qu.: 93.125 3rd Qu.:21.70 ## Max. :100.000 Max. :26.60 1.2.2 Visual summaries Scatterplot matrix (default plot of a data.frame): plot(swiss) # or pairs(swiss) Scatterplot of two dimensions plot(swiss[,c(&quot;Education&quot;, &quot;Fertility&quot;)]) # or plot(swiss[4,1]) # or plot(swiss$Education, swiss$Fertility) # or plot(swiss$Fertility ~ swiss$Education) Smoothed Scatterplot of two dimensions smoothScatter(swiss$Fertility ~ swiss$Examination) Scatterplot with a loess (locally weighted polynomial regression) scatter.smooth(swiss$Fertility ~ swiss$Agriculture) 1.2.3 Distribution plots Histograms: hist(swiss$Catholic) Stem-and-Leaf Plots: stem(swiss$Fertility) ## ## The decimal point is 1 digit(s) to the right of the | ## ## 3 | 5 ## 4 | 35 ## 5 | 46778 ## 6 | 124455556678899 ## 7 | 01223346677899 ## 8 | 0233467 ## 9 | 223 Kernel density plot (and add a rug showing where observation occur): plot(density(swiss$Fertility)) rug(swiss$Fertility) Boxplots: boxplot(swiss) 1.2.3.1 More complicated charts Conditioning plots: coplot(swiss$Fertility ~ swiss$Examination | as.factor(swiss$Catholic &gt; 50)) Star plots (half-star plots here): stars(swiss, key.loc = c(15,1), flip.labels = FALSE, full = FALSE) 1.3 Assignment Choose a dataset from datasets (library(help = &quot;datasets&quot;) will show you a list) and create 5 charts in an R Markdown file from the example charts above. Run the following command to see what else is available in the base R graphics package: demo(graphics) "],
["read-data.html", "Lecture 2 Reading data 2.1 Data Source: 2.2 read_csv 2.3 dplyr 2.4 First Look at ggplot2 2.5 Heatmaps 2.6 Hexbins 2.7 Other topics from this dataset", " Lecture 2 Reading data The first step in analyzing data with R is reading data into it. This lesson focuses on reading data, manipulating it with dplyr and a few summary visualizations. 2.1 Data Source: The US Census Bureau has a large selection of data on the population of the United States. We’ll take a look at the 1-year American Community Survey results for the state of Hawaii. Hawaii Population Records The data dictionary 2.2 read_csv To read in the downloaded file, we’ll use the readr package, which you can install by installing tidyverse. library(readr) # also in library(tidyverse) pop_hi &lt;- read_csv(&quot;data/csv_phi.zip&quot;) ## Multiple files in zip: reading &#39;ss15phi.csv&#39; ## Parsed with column specification: ## cols( ## .default = col_integer(), ## RT = col_character(), ## SPORDER = col_character(), ## PUMA = col_character(), ## PWGTP = col_character(), ## AGEP = col_character(), ## INTP = col_character(), ## JWMNP = col_character(), ## JWRIP = col_character(), ## JWTR = col_character(), ## OIP = col_character(), ## PAP = col_character(), ## RELP = col_character(), ## RETP = col_character(), ## SCHG = col_character(), ## SCHL = col_character(), ## SEMP = col_character(), ## SSIP = col_character(), ## SSP = col_character(), ## WAGP = col_character(), ## WKHP = col_character() ## # ... with 99 more columns ## ) ## See spec(...) for full column specifications. pop_hi ## # A tibble: 14,124 x 284 ## RT SERIALNO SPORDER PUMA ST ADJINC PWGTP AGEP CIT CITWP ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 P 21 01 00303 15 1001264 00078 63 1 NA ## 2 P 158 01 00306 15 1001264 00056 54 1 NA ## 3 P 267 01 00100 15 1001264 00059 52 5 NA ## 4 P 267 02 00100 15 1001264 00071 56 5 NA ## 5 P 267 03 00100 15 1001264 00102 25 5 NA ## 6 P 267 04 00100 15 1001264 00155 22 5 NA ## 7 P 267 05 00100 15 1001264 00074 15 5 NA ## 8 P 351 01 00100 15 1001264 00307 32 4 2005 ## 9 P 351 02 00100 15 1001264 00578 02 1 NA ## 10 P 470 01 00200 15 1001264 00041 79 1 NA ## # ... with 14,114 more rows, and 274 more variables: COW &lt;int&gt;, ## # DDRS &lt;int&gt;, DEAR &lt;int&gt;, DEYE &lt;int&gt;, DOUT &lt;int&gt;, DPHY &lt;int&gt;, ## # DRAT &lt;int&gt;, DRATX &lt;int&gt;, DREM &lt;int&gt;, ENG &lt;int&gt;, FER &lt;int&gt;, GCL &lt;int&gt;, ## # GCM &lt;int&gt;, GCR &lt;int&gt;, HINS1 &lt;int&gt;, HINS2 &lt;int&gt;, HINS3 &lt;int&gt;, ## # HINS4 &lt;int&gt;, HINS5 &lt;int&gt;, HINS6 &lt;int&gt;, HINS7 &lt;int&gt;, INTP &lt;chr&gt;, ## # JWMNP &lt;chr&gt;, JWRIP &lt;chr&gt;, JWTR &lt;chr&gt;, LANX &lt;int&gt;, MAR &lt;int&gt;, ## # MARHD &lt;int&gt;, MARHM &lt;int&gt;, MARHT &lt;int&gt;, MARHW &lt;int&gt;, MARHYP &lt;int&gt;, ## # MIG &lt;int&gt;, MIL &lt;int&gt;, MLPA &lt;int&gt;, MLPB &lt;int&gt;, MLPCD &lt;int&gt;, MLPE &lt;int&gt;, ## # MLPFG &lt;int&gt;, MLPH &lt;int&gt;, MLPI &lt;int&gt;, MLPJ &lt;int&gt;, MLPK &lt;int&gt;, ## # NWAB &lt;int&gt;, NWAV &lt;int&gt;, NWLA &lt;int&gt;, NWLK &lt;int&gt;, NWRE &lt;int&gt;, OIP &lt;chr&gt;, ## # PAP &lt;chr&gt;, RELP &lt;chr&gt;, RETP &lt;chr&gt;, SCH &lt;int&gt;, SCHG &lt;chr&gt;, SCHL &lt;chr&gt;, ## # SEMP &lt;chr&gt;, SEX &lt;int&gt;, SSIP &lt;chr&gt;, SSP &lt;chr&gt;, WAGP &lt;chr&gt;, WKHP &lt;chr&gt;, ## # WKL &lt;int&gt;, WKW &lt;int&gt;, WRK &lt;int&gt;, YOEP &lt;int&gt;, ANC &lt;int&gt;, ANC1P &lt;chr&gt;, ## # ANC2P &lt;chr&gt;, DECADE &lt;int&gt;, DIS &lt;int&gt;, DRIVESP &lt;int&gt;, ESP &lt;int&gt;, ## # ESR &lt;int&gt;, FHICOVP &lt;int&gt;, FOD1P &lt;int&gt;, FOD2P &lt;int&gt;, HICOV &lt;int&gt;, ## # HISP &lt;chr&gt;, INDP &lt;chr&gt;, JWAP &lt;chr&gt;, JWDP &lt;chr&gt;, LANP &lt;int&gt;, ## # MIGPUMA &lt;chr&gt;, MIGSP &lt;chr&gt;, MSP &lt;int&gt;, NAICSP &lt;chr&gt;, NATIVITY &lt;int&gt;, ## # NOP &lt;int&gt;, OC &lt;int&gt;, OCCP &lt;chr&gt;, PAOC &lt;int&gt;, PERNP &lt;chr&gt;, PINCP &lt;chr&gt;, ## # POBP &lt;chr&gt;, POVPIP &lt;chr&gt;, POWPUMA &lt;chr&gt;, POWSP &lt;chr&gt;, PRIVCOV &lt;int&gt;, ## # PUBCOV &lt;int&gt;, QTRBIR &lt;int&gt;, ... 2.3 dplyr Using the data dictionary we can identify some interesting variables. PERSON RECORD RT 1 Record Type P .Person Record AGEP 2 Age 00 .Under 1 year 01..99 .1 to 99 years (Top-coded***) COW 1 Class of worker b .N/A (less than 16 years old/NILF who last .worked more than 5 years ago or never worked) 1 .Employee of a private for-profit company or .business, or of an individual, for wages, .salary, or commissions 2 .Employee of a private not-for-profit, .tax-exempt, or charitable organization 3 .Local government employee (city, county, etc.) 4 .State government employee 5 .Federal government employee 6 .Self-employed in own not incorporated .business, professional practice, or farm 7 .Self-employed in own incorporated .business, professional practice or farm 8 .Working without pay in family business or farm 9 .Unemployed and last worked 5 years ago or earlier or never .worked SCHL 2 Educational attainment bb .N/A (less than 3 years old) 01 .No schooling completed 02 .Nursery school, preschool 03 .Kindergarten 04 .Grade 1 05 .Grade 2 06 .Grade 3 07 .Grade 4 08 .Grade 5 09 .Grade 6 10 .Grade 7 11 .Grade 8 12 .Grade 9 13 .Grade 10 14 .Grade 11 15 .12th grade - no diploma 16 .Regular high school diploma 17 .GED or alternative credential 18 .Some college, but less than 1 year 19 .1 or more years of college credit, no degree 20 .Associate&#39;s degree 21 .Bachelor&#39;s degree 22 .Master&#39;s degree 23 .Professional degree beyond a bachelor&#39;s degree 24 .Doctorate degree WAGP 6 Wages or salary income past 12 months bbbbbb .N/A (less than 15 years old) 000000 .None 000001..999999 .$1 to 999999 (Rounded and top-coded) Note: Use ADJINC to adjust WAGP to constant dollars. WKHP 2 Usual hours worked per week past 12 months bb .N/A (less than 16 years old/did not work .during the past 12 months) 01..98 .1 to 98 usual hours 99 .99 or more usual hours WKW 1 Weeks worked during past 12 months b .N/A (less than 16 years old/did not work .during the past 12 months) 1 .50 to 52 weeks worked during past 12 months 2 .48 to 49 weeks worked during past 12 months 3 .40 to 47 weeks worked during past 12 months 4 .27 to 39 weeks worked during past 12 months 5 .14 to 26 weeks worked during past 12 months 6 .less than 14 weeks worked during past 12 months ESR 1 Employment status recode b .N/A (less than 16 years old) 1 .Civilian employed, at work 2 .Civilian employed, with a job but not at work 3 .Unemployed 4 .Armed forces, at work 5 .Armed forces, with a job but not at work 6 .Not in labor force PERNP 7 Total person&#39;s earnings bbbbbbb .N/A (less than 15 years old) 0000000 .No earnings -010000 .Loss of $10000 or more (Rounded &amp; bottom-coded .components) -000001..-009999 .Loss $1 to $9999 (Rounded components) 0000001 .$1 or break even 0000002..9999999 .$2 to $9999999 (Rounded &amp; top-coded components) Note: Use ADJINC to adjust PERNP to constant dollars. PINCP 7 Total person&#39;s income (signed) bbbbbbb .N/A (less than 15 years old) 0000000 .None -019999 .Loss of $19999 or more (Rounded &amp; bottom-coded .components) -000001..-019998 .Loss $1 to $19998 (Rounded components) 0000001 .$1 or break even 0000002..9999999 .$2 to $9999999 (Rounded &amp; top-coded components) Note: Use ADJINC to adjust PINCP to constant dollars. Let’s focus on employed civilians (ESR either 1 or 2) working full time (WKHP &gt; 32) for close to the entire year (WKW either 1 or 2). library(dplyr) # also in library(tidyverse) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union pop_hi &lt;- pop_hi %&gt;% filter( ESR %in% c(1, 2), as.numeric(WKHP) &gt; 32, WKW %in% c(1, 2) ) If you are unsure if a column that you want to treat as numeric contains letters, you can run the following command to get a list of the values containing letters: grep(&quot;[[:alpha:]]&quot;, pop_hi$WKHP, value = TRUE) ## character(0) We can use two functions to add new columns (or change existing ones). mutate() adds columns and keeps the previous columns transmute() adds columns and removes the previous columns This time we want to drop the columns we don’t mention. pop_hi &lt;- pop_hi %&gt;% transmute( age = as.numeric(AGEP), worker_class = factor(COW, labels = c( &quot;for-profit&quot;, &quot;not-for-profit&quot;, &quot;local government&quot;, &quot;state government&quot;, &quot;federal government&quot;, &quot;self-employed not incorporated&quot;, &quot;self-employed incorporated&quot;, &quot;family business no pay&quot; )), school = SCHL, wages = as.numeric(WAGP), top_coded_wages = WAGP == 999999 ) Creating a custom factor variable for educational attainment: education_levels &lt;- c(&quot;less than HS&quot;, &quot;HS&quot;, &quot;associates&quot;, &quot;bachelors&quot;, &quot;masters&quot;, &quot;doctorate&quot;) pop_hi$education &lt;- NA pop_hi[pop_hi$school &lt; 16,]$education &lt;- &quot;less than HS&quot; pop_hi[pop_hi$school &gt; 16 &amp; pop_hi$school &lt; 20,]$education &lt;- &quot;HS&quot; pop_hi[pop_hi$school == 20,]$education &lt;- &quot;associates&quot; pop_hi[pop_hi$school == 21,]$education &lt;- &quot;bachelors&quot; pop_hi[pop_hi$school %in% c(22, 23),]$education &lt;- &quot;masters&quot; pop_hi[pop_hi$school == 24,]$education &lt;- &quot;doctorate&quot; pop_hi &lt;- pop_hi %&gt;% mutate(education = factor(education, levels = education_levels)) 2.4 First Look at ggplot2 See the ggplot2 documentation for details and inspiration. library(ggplot2) # also in library(tidyverse) ggplot(pop_hi, aes(age, wages)) + geom_point() Income has a skewed distribution, so it is often presented/analyzed in logs. Here’s how to modify the above chart to display income in logs: pop_hi &lt;- pop_hi %&gt;% mutate(log_safe_wages = ifelse(wages == 0, 1, wages)) pop_hi %&gt;% ggplot(aes(age, log_safe_wages)) + geom_point() + scale_y_log10() 2.5 Heatmaps Heatmaps allow you to get a sense of the concetration of observations in regions where there are many overlapping points: ggplot(pop_hi, aes(age, log_safe_wages)) + geom_bin2d() + scale_y_log10() 2.6 Hexbins Hexbins use hexagons instead of squares, which helps avoid the rectangular sections in heatmaps that may misrepresent your data. You will need to install the hexbin package. install.packages(&quot;hexbin&quot;) pop_hi %&gt;% filter(wages &gt; 10000, wages &lt; 300000) %&gt;% ggplot(aes(age, wages)) + geom_hex() + scale_x_log10() Here we can see that inequality in wages for workers increases with age. 2.7 Other topics from this dataset This dataset includes information on the majors for degree holders and the industry codes. You could use that additional information to ask how well targeted majors are to particular industries and how incomes vary across choice of major. Because of the size of the data in the Hawaii sample, it would be better to ask some of these questions at the national level. Go to the ACS PUMS documentation page for more information. "],
["anscombe.html", "Lecture 3 Anscombe’s Quartet 3.1 Prep the data 3.2 Numeric summary 3.3 Visual summary 3.4 The Datasaurus Dozen", " Lecture 3 Anscombe’s Quartet Anscombe quartet emphasizes the need to move beyond basic numerical summaries of your data. The anscombe dataset has four sets of x and y variables with very similar summaries, but distinct visual patterns 3.1 Prep the data anscombe ## x1 x2 x3 x4 y1 y2 y3 y4 ## 1 10 10 10 8 8.04 9.14 7.46 6.58 ## 2 8 8 8 8 6.95 8.14 6.77 5.76 ## 3 13 13 13 8 7.58 8.74 12.74 7.71 ## 4 9 9 9 8 8.81 8.77 7.11 8.84 ## 5 11 11 11 8 8.33 9.26 7.81 8.47 ## 6 14 14 14 8 9.96 8.10 8.84 7.04 ## 7 6 6 6 8 7.24 6.13 6.08 5.25 ## 8 4 4 4 19 4.26 3.10 5.39 12.50 ## 9 12 12 12 8 10.84 9.13 8.15 5.56 ## 10 7 7 7 8 4.82 7.26 6.42 7.91 ## 11 5 5 5 8 5.68 4.74 5.73 6.89 First we’ll use tidyr to reshape the anscombe dataset to make it easier to work with. library(tidyr) tidy_anscombe &lt;- anscombe %&gt;% mutate(id = row_number()) %&gt;% gather(key = key, value = value, everything(), -id) tidy_anscombe ## id key value ## 1 1 x1 10.00 ## 2 2 x1 8.00 ## 3 3 x1 13.00 ## 4 4 x1 9.00 ## 5 5 x1 11.00 ## 6 6 x1 14.00 ## 7 7 x1 6.00 ## 8 8 x1 4.00 ## 9 9 x1 12.00 ## 10 10 x1 7.00 ## 11 11 x1 5.00 ## 12 1 x2 10.00 ## 13 2 x2 8.00 ## 14 3 x2 13.00 ## 15 4 x2 9.00 ## 16 5 x2 11.00 ## 17 6 x2 14.00 ## 18 7 x2 6.00 ## 19 8 x2 4.00 ## 20 9 x2 12.00 ## 21 10 x2 7.00 ## 22 11 x2 5.00 ## 23 1 x3 10.00 ## 24 2 x3 8.00 ## 25 3 x3 13.00 ## 26 4 x3 9.00 ## 27 5 x3 11.00 ## 28 6 x3 14.00 ## 29 7 x3 6.00 ## 30 8 x3 4.00 ## 31 9 x3 12.00 ## 32 10 x3 7.00 ## 33 11 x3 5.00 ## 34 1 x4 8.00 ## 35 2 x4 8.00 ## 36 3 x4 8.00 ## 37 4 x4 8.00 ## 38 5 x4 8.00 ## 39 6 x4 8.00 ## 40 7 x4 8.00 ## 41 8 x4 19.00 ## 42 9 x4 8.00 ## 43 10 x4 8.00 ## 44 11 x4 8.00 ## 45 1 y1 8.04 ## 46 2 y1 6.95 ## 47 3 y1 7.58 ## 48 4 y1 8.81 ## 49 5 y1 8.33 ## 50 6 y1 9.96 ## 51 7 y1 7.24 ## 52 8 y1 4.26 ## 53 9 y1 10.84 ## 54 10 y1 4.82 ## 55 11 y1 5.68 ## 56 1 y2 9.14 ## 57 2 y2 8.14 ## 58 3 y2 8.74 ## 59 4 y2 8.77 ## 60 5 y2 9.26 ## 61 6 y2 8.10 ## 62 7 y2 6.13 ## 63 8 y2 3.10 ## 64 9 y2 9.13 ## 65 10 y2 7.26 ## 66 11 y2 4.74 ## 67 1 y3 7.46 ## 68 2 y3 6.77 ## 69 3 y3 12.74 ## 70 4 y3 7.11 ## 71 5 y3 7.81 ## 72 6 y3 8.84 ## 73 7 y3 6.08 ## 74 8 y3 5.39 ## 75 9 y3 8.15 ## 76 10 y3 6.42 ## 77 11 y3 5.73 ## 78 1 y4 6.58 ## 79 2 y4 5.76 ## 80 3 y4 7.71 ## 81 4 y4 8.84 ## 82 5 y4 8.47 ## 83 6 y4 7.04 ## 84 7 y4 5.25 ## 85 8 y4 12.50 ## 86 9 y4 5.56 ## 87 10 y4 7.91 ## 88 11 y4 6.89 tidy_anscombe &lt;- tidy_anscombe %&gt;% separate(key, c(&quot;x_or_y&quot;, &quot;series&quot;), 1) tidy_anscombe ## id x_or_y series value ## 1 1 x 1 10.00 ## 2 2 x 1 8.00 ## 3 3 x 1 13.00 ## 4 4 x 1 9.00 ## 5 5 x 1 11.00 ## 6 6 x 1 14.00 ## 7 7 x 1 6.00 ## 8 8 x 1 4.00 ## 9 9 x 1 12.00 ## 10 10 x 1 7.00 ## 11 11 x 1 5.00 ## 12 1 x 2 10.00 ## 13 2 x 2 8.00 ## 14 3 x 2 13.00 ## 15 4 x 2 9.00 ## 16 5 x 2 11.00 ## 17 6 x 2 14.00 ## 18 7 x 2 6.00 ## 19 8 x 2 4.00 ## 20 9 x 2 12.00 ## 21 10 x 2 7.00 ## 22 11 x 2 5.00 ## 23 1 x 3 10.00 ## 24 2 x 3 8.00 ## 25 3 x 3 13.00 ## 26 4 x 3 9.00 ## 27 5 x 3 11.00 ## 28 6 x 3 14.00 ## 29 7 x 3 6.00 ## 30 8 x 3 4.00 ## 31 9 x 3 12.00 ## 32 10 x 3 7.00 ## 33 11 x 3 5.00 ## 34 1 x 4 8.00 ## 35 2 x 4 8.00 ## 36 3 x 4 8.00 ## 37 4 x 4 8.00 ## 38 5 x 4 8.00 ## 39 6 x 4 8.00 ## 40 7 x 4 8.00 ## 41 8 x 4 19.00 ## 42 9 x 4 8.00 ## 43 10 x 4 8.00 ## 44 11 x 4 8.00 ## 45 1 y 1 8.04 ## 46 2 y 1 6.95 ## 47 3 y 1 7.58 ## 48 4 y 1 8.81 ## 49 5 y 1 8.33 ## 50 6 y 1 9.96 ## 51 7 y 1 7.24 ## 52 8 y 1 4.26 ## 53 9 y 1 10.84 ## 54 10 y 1 4.82 ## 55 11 y 1 5.68 ## 56 1 y 2 9.14 ## 57 2 y 2 8.14 ## 58 3 y 2 8.74 ## 59 4 y 2 8.77 ## 60 5 y 2 9.26 ## 61 6 y 2 8.10 ## 62 7 y 2 6.13 ## 63 8 y 2 3.10 ## 64 9 y 2 9.13 ## 65 10 y 2 7.26 ## 66 11 y 2 4.74 ## 67 1 y 3 7.46 ## 68 2 y 3 6.77 ## 69 3 y 3 12.74 ## 70 4 y 3 7.11 ## 71 5 y 3 7.81 ## 72 6 y 3 8.84 ## 73 7 y 3 6.08 ## 74 8 y 3 5.39 ## 75 9 y 3 8.15 ## 76 10 y 3 6.42 ## 77 11 y 3 5.73 ## 78 1 y 4 6.58 ## 79 2 y 4 5.76 ## 80 3 y 4 7.71 ## 81 4 y 4 8.84 ## 82 5 y 4 8.47 ## 83 6 y 4 7.04 ## 84 7 y 4 5.25 ## 85 8 y 4 12.50 ## 86 9 y 4 5.56 ## 87 10 y 4 7.91 ## 88 11 y 4 6.89 tidy_anscombe &lt;- tidy_anscombe %&gt;% spread(x_or_y, value) tidy_anscombe ## id series x y ## 1 1 1 10 8.04 ## 2 1 2 10 9.14 ## 3 1 3 10 7.46 ## 4 1 4 8 6.58 ## 5 2 1 8 6.95 ## 6 2 2 8 8.14 ## 7 2 3 8 6.77 ## 8 2 4 8 5.76 ## 9 3 1 13 7.58 ## 10 3 2 13 8.74 ## 11 3 3 13 12.74 ## 12 3 4 8 7.71 ## 13 4 1 9 8.81 ## 14 4 2 9 8.77 ## 15 4 3 9 7.11 ## 16 4 4 8 8.84 ## 17 5 1 11 8.33 ## 18 5 2 11 9.26 ## 19 5 3 11 7.81 ## 20 5 4 8 8.47 ## 21 6 1 14 9.96 ## 22 6 2 14 8.10 ## 23 6 3 14 8.84 ## 24 6 4 8 7.04 ## 25 7 1 6 7.24 ## 26 7 2 6 6.13 ## 27 7 3 6 6.08 ## 28 7 4 8 5.25 ## 29 8 1 4 4.26 ## 30 8 2 4 3.10 ## 31 8 3 4 5.39 ## 32 8 4 19 12.50 ## 33 9 1 12 10.84 ## 34 9 2 12 9.13 ## 35 9 3 12 8.15 ## 36 9 4 8 5.56 ## 37 10 1 7 4.82 ## 38 10 2 7 7.26 ## 39 10 3 7 6.42 ## 40 10 4 8 7.91 ## 41 11 1 5 5.68 ## 42 11 2 5 4.74 ## 43 11 3 5 5.73 ## 44 11 4 8 6.89 3.2 Numeric summary tidy_anscombe %&gt;% group_by(series) %&gt;% summarise( mean_x = mean(x), mean_y = mean(y), sd_x = sd(x), sd_y = sd(y), cor = cor(x, y) ) ## # A tibble: 4 x 6 ## series mean_x mean_y sd_x sd_y cor ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 9 7.500909 3.316625 2.031568 0.8164205 ## 2 2 9 7.500909 3.316625 2.031657 0.8162365 ## 3 3 9 7.500000 3.316625 2.030424 0.8162867 ## 4 4 9 7.500909 3.316625 2.030579 0.8165214 3.3 Visual summary While the visual summaries suggest very similar datasets, the visual summaries help identify the differences: library(ggplot2) tidy_anscombe %&gt;% ggplot(aes(x, y)) + geom_point() + facet_wrap(~ series) + coord_fixed() 3.4 The Datasaurus Dozen The Datasaurus Dozen is a set of series, like the Anscombe’s quartet with similar numerical summaries and radically different visual summaries. See a great discussion of this dataset by the creators, Justin Matejka and George Fitzmaurice here Download the data here and move the DatasaurusDozen.tsv file into your data folder. datasaurus &lt;- read_tsv(&quot;data/DatasaurusDozen.tsv&quot;) ## Parsed with column specification: ## cols( ## dataset = col_character(), ## x = col_double(), ## y = col_double() ## ) datasaurus %&gt;% group_by(dataset) %&gt;% summarise( mean_x = mean(x), mean_y = mean(y), sd_x = sd(x), sd_y = sd(y), cor = cor(x, y) ) ## # A tibble: 13 x 6 ## dataset mean_x mean_y sd_x sd_y cor ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 away 54.26610 47.83472 16.76982 26.93974 -0.06412835 ## 2 bullseye 54.26873 47.83082 16.76924 26.93573 -0.06858639 ## 3 circle 54.26732 47.83772 16.76001 26.93004 -0.06834336 ## 4 dino 54.26327 47.83225 16.76514 26.93540 -0.06447185 ## 5 dots 54.26030 47.83983 16.76774 26.93019 -0.06034144 ## 6 h_lines 54.26144 47.83025 16.76590 26.93988 -0.06171484 ## 7 high_lines 54.26881 47.83545 16.76670 26.94000 -0.06850422 ## 8 slant_down 54.26785 47.83590 16.76676 26.93610 -0.06897974 ## 9 slant_up 54.26588 47.83150 16.76885 26.93861 -0.06860921 ## 10 star 54.26734 47.83955 16.76896 26.93027 -0.06296110 ## 11 v_lines 54.26993 47.83699 16.76996 26.93768 -0.06944557 ## 12 wide_lines 54.26692 47.83160 16.77000 26.93790 -0.06657523 ## 13 x_shape 54.26015 47.83972 16.76996 26.93000 -0.06558334 Visual summaries datasaurus %&gt;% ggplot(aes(x, y)) + geom_point() + facet_wrap(~ dataset, ncol = 6) + coord_fixed() "],
["trifecta.html", "Lecture 4 How to Judge Visualizations 4.1 Pre-class assignment 4.2 Lecture 4.3 Assignment", " Lecture 4 How to Judge Visualizations 4.1 Pre-class assignment Read Junk Charts Trifecta Checkup 4.2 Lecture Kaiser Fung has a useful framework for judging the quality of data visualizations. He calls his framework the Junk Charts Trifecta Checkup. To make that shorter, we’ll just call this framework the Trifecta Checkup. This framework boils down to the following 3 questions: What is the question? What does the data say? What does the visual say? Each visualization can be labeled according to which question(s) it does not answer well. For example, a chart with a meaningful question and relevant data, but ineffective visuals, would be labeled v. A chart that gets the data and question wrong, but has a nice visual, would be labeled qd. 4.3 Assignment Assign a label to each of the following visualizations based on the Trifecta Checkup. Provide at least one sentence per question defending your chosen labels. "]
]
