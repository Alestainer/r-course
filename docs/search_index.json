[
["index.html", "R for Data Analysis and Visualization Syllabus Office Hours Student Learning Objectives Resources Course Requirements Individual Project Schedule Other Resources", " R for Data Analysis and Visualization ECON 396 (Fall 2017) TR 10:30-11:45, Webster Hall 112 Jonathan Page 2017-07-20 Syllabus Office Hours Monday 2-3 PM and Tuesday 3-4 PM, or by appointment, Saunders 509, jrpage at hawaii dot edu. Student Learning Objectives To be familiar with standard techniques for visualizing data, including heat maps, contour plots, (look at the list in the Data Visualization book) To be able to transform raw data into formats suitable for analysis To be able to perform basic exploratory analysis To be able to create data visualizations There is no prerequisite for this course. Resources Required Introductory Statistics with Randomization and Simulation: Available as a free PDF (https://www.openintro.org/stat/textbook.php?stat_book=isrs) or for $8.49 on Amazon. R Graphics Cookbook Recommended: RStudio Cheat Sheets Course Requirements Grades for this course will be based on weekly assignments (30%), project assignments (30%), the project proposal (5%), the final project deliverable (15%), and final project presentation participation (20%). Weekly assignments (30%) Weekly assignments are short R excercises. Each exercise should take no longer than 15 minutes. You will typically be given time to complete the exercise in class the day the assignment is given. The assignment will be in the form of R Markdown file (*.Rmd). You will submit the completed assignments via classroom.google.com by the following class period. Individual Project Project assignments (30%) Each week, leading up to the project proposal, you will be given an assignment that is designed to provide you with an organized workflow for approaching new data science projects. Project assignments are submitted via classroom.google.com, with the exception of the two presentations Project proposal presentation (5%) This presentation should be less than 2 minutes. You simply need to communicate the core question your project seeks to answer and the dataset(s) you will be using to answer this question. Final project (15%) The final project will be an R Markdown document which communicates your project question, the data you used, and your results. Final project presentation participation (20%) Your final project participation grade is based on a combination of your own presentation and the feedback you provide to your classmates. Schedule The following schedule is tentative and subject to change. Week 1 R Intro to R and RStudio; Histograms, scatterplots, summary statistics Topic Data sources overview Project Assignment Indentify interesting datasets and questions Week 2 R read_csv, dplyr basics, heatmaps, hexbins Topic Anscombe’s Quartet Project Assignment Choose question and dataset for your project Week 3 R ggplot facets, bubble plots, transparency Topic Effective Data Visualization Project Assignment Write description of your question Week 4 R geom_smooth, abline, vline, hline Topic Time series analysis Project Assignment Write description of your dataset(s) Week 5 R ggplot2 Extensions and Scatterplot Matrices (GGally) Topic JunkCharts Trifecta Checkup Project Assignment Create 2 descriptive plots of your datasets(s) Week 6 R Boxplots, violin plots Topic Project Assignment Write a description of the data cleaning required for your project Week 7 R Spatial Visualizations with geom_spoke, gganimate, and GGally::glyphs Topic Project Assignment Write a description of your planned approach Week 8 R geom_area, geom_ribbon Topic Project Proposal Description Project Assignment Work on project proposal presentation Week 9 R jitter, rug, aesthetics Project Assignment Present project proposal (&lt;2 Minutes) Week 10 R themes, ggthemes extension, labels, color scales Topic Project Assignment Work on final project Week 11 R polar coordinates, ggradar extension Topic Project Assignment Work on final project (cont.) Week 12 R word/text analysis Topic Project Assignment Work on final project (cont.) Week 13 R gganimate Topic Project Assignment Work on final project (cont.) Week 14 R git and GitHub for R Topic Project Assignment Week 15 R networks, geomnet extension Topic Final Project presentations Project Assignment Other Resources There are many useful resources you should be aware of while going through this course: RStudio’s List of Useful R Packages Statistics Variance Explained R for Data Science - Grolemund and Wickham Visualization FlowingData Junk Charts Courses Gary King - Quantitative Research Methodology John Stasko - Information Visualization Jenny Bryan - Data wrangling, exploration, and analysis with R Books Econometrics in R Using R for Data Analysis and Graphics Papers Embedded Plots "],
["individual-project-1.html", "Individual Project Project assignments Project proposal presentation Final project RMarkdown Final project presentation participation", " Individual Project Project assignments Each week, leading up to the project proposal, you will be given an assignment that is designed to provide you with an organized workflow for approaching new data science projects. Project assignments are submitted via classroom.google.com, with the exception of the two presentations Project proposal presentation This presentation should be less than 2 minutes. You simply need to communicate the core question your project seeks to answer and the dataset(s) you will be using to answer this question. Final project RMarkdown The final project will be an R Markdown document which communicates your project question, the data you used, and your results. Final project presentation participation Your final project participation grade is based on a combination of your own presentation and the feedback you provide to your classmates. "],
["intro.html", "Lecture 1 R Basics 1.1 R Markdown 1.2 Working with data already loaded into R 1.3 Assignment", " Lecture 1 R Basics Before we begin, make sure you have R and RStudio installed. 1.1 R Markdown Throughout this course, R Markdown will make our lives easier. Make sure that the rmarkdown library is installed: install.packages(&quot;rmarkdown&quot;) For each assignment, you will create an R Markdown file (*.Rmd) and submit that file by the following class session using classroom.google.com 1.2 Working with data already loaded into R Base R comes with a set of sample data that is useful for illustrating techniques in R. Run the following command to see a list of the datasets in the core library datasets: library(help = &quot;datasets&quot;) These datasets are accessible automatically. We’ll start with the Swiss Fertility and Socioeconomic Inicators (1888) dataset. See a description of the dataset by using the help command, either ?swiss or help(swiss). This dataset is technically a data.frame, which you can see by using the command class(swiss). For more information on data.frames take a look at the documentation(help(data.frame)) 1.2.1 Numeric summaries Here are a few ways we can summarize a dataset: head() shows us the first six rows of a data.frame. head(swiss) ## Fertility Agriculture Examination Education Catholic ## Courtelary 80.2 17.0 15 12 9.96 ## Delemont 83.1 45.1 6 9 84.84 ## Franches-Mnt 92.5 39.7 5 5 93.40 ## Moutier 85.8 36.5 12 7 33.77 ## Neuveville 76.9 43.5 17 15 5.16 ## Porrentruy 76.1 35.3 9 7 90.57 ## Infant.Mortality ## Courtelary 22.2 ## Delemont 22.2 ## Franches-Mnt 20.2 ## Moutier 20.3 ## Neuveville 20.6 ## Porrentruy 26.6 summary() provides summary statistics for each column in a data.frame. summary(swiss) ## Fertility Agriculture Examination Education ## Min. :35.00 Min. : 1.20 Min. : 3.00 Min. : 1.00 ## 1st Qu.:64.70 1st Qu.:35.90 1st Qu.:12.00 1st Qu.: 6.00 ## Median :70.40 Median :54.10 Median :16.00 Median : 8.00 ## Mean :70.14 Mean :50.66 Mean :16.49 Mean :10.98 ## 3rd Qu.:78.45 3rd Qu.:67.65 3rd Qu.:22.00 3rd Qu.:12.00 ## Max. :92.50 Max. :89.70 Max. :37.00 Max. :53.00 ## Catholic Infant.Mortality ## Min. : 2.150 Min. :10.80 ## 1st Qu.: 5.195 1st Qu.:18.15 ## Median : 15.140 Median :20.00 ## Mean : 41.144 Mean :19.94 ## 3rd Qu.: 93.125 3rd Qu.:21.70 ## Max. :100.000 Max. :26.60 1.2.2 Visual summaries Scatterplot matrix (default plot of a data.frame): plot(swiss) # or pairs(swiss) Scatterplot of two dimensions plot(swiss[,c(&quot;Education&quot;, &quot;Fertility&quot;)]) # or plot(swiss[4,1]) # or plot(swiss$Education, swiss$Fertility) # or plot(swiss$Fertility ~ swiss$Education) Smoothed Scatterplot of two dimensions smoothScatter(swiss$Fertility ~ swiss$Examination) Scatterplot with a loess (locally weighted polynomial regression) scatter.smooth(swiss$Fertility ~ swiss$Agriculture) 1.2.3 Distribution plots Histograms: hist(swiss$Catholic) Stem-and-Leaf Plots: stem(swiss$Fertility) ## ## The decimal point is 1 digit(s) to the right of the | ## ## 3 | 5 ## 4 | 35 ## 5 | 46778 ## 6 | 124455556678899 ## 7 | 01223346677899 ## 8 | 0233467 ## 9 | 223 Kernel density plot (and add a rug showing where observation occur): plot(density(swiss$Fertility)) rug(swiss$Fertility) Boxplots: boxplot(swiss) 1.2.3.1 More complicated charts Conditioning plots: coplot(swiss$Fertility ~ swiss$Examination | as.factor(swiss$Catholic &gt; 50)) Star plots (half-star plots here): stars(swiss, key.loc = c(15,1), flip.labels = FALSE, full = FALSE) 1.3 Assignment Choose a dataset from datasets (library(help = &quot;datasets&quot;) will show you a list) and create 5 charts in an R Markdown file from the example charts above. Run the following command to see what else is available in the base R graphics package: demo(graphics) "],
["read-data.html", "Lecture 2 Reading data 2.1 Data Source: 2.2 read_csv 2.3 dplyr 2.4 First Look at ggplot2 2.5 Heatmaps 2.6 Hexbins 2.7 Other topics from this dataset 2.8 Assignment", " Lecture 2 Reading data The first step in analyzing data with R is reading data into it. This lesson focuses on reading data, manipulating it with dplyr and a few summary visualizations. 2.1 Data Source: The US Census Bureau has a large selection of data on the population of the United States. The public-use micro surveys (PUMS) are available from the following link: https://www.census.gov/programs-surveys/acs/data/pums.html We’ll take a look at the 1-year American Community Survey results for the state of Hawaii. Hawaii Population Records The data dictionary 2.2 read_csv To read in the downloaded file, we’ll use the readr package, which you can install by installing tidyverse. library(readr) # also in library(tidyverse) pop_hi &lt;- read_csv(&quot;data/csv_phi.zip&quot;) pop_hi ## # A tibble: 14,124 × 284 ## RT SERIALNO SPORDER PUMA ST ADJINC PWGTP AGEP CIT CITWP ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 P 21 01 00303 15 1001264 00078 63 1 NA ## 2 P 158 01 00306 15 1001264 00056 54 1 NA ## 3 P 267 01 00100 15 1001264 00059 52 5 NA ## 4 P 267 02 00100 15 1001264 00071 56 5 NA ## 5 P 267 03 00100 15 1001264 00102 25 5 NA ## 6 P 267 04 00100 15 1001264 00155 22 5 NA ## 7 P 267 05 00100 15 1001264 00074 15 5 NA ## 8 P 351 01 00100 15 1001264 00307 32 4 2005 ## 9 P 351 02 00100 15 1001264 00578 02 1 NA ## 10 P 470 01 00200 15 1001264 00041 79 1 NA ## # ... with 14,114 more rows, and 274 more variables: COW &lt;int&gt;, ## # DDRS &lt;int&gt;, DEAR &lt;int&gt;, DEYE &lt;int&gt;, DOUT &lt;int&gt;, DPHY &lt;int&gt;, ## # DRAT &lt;int&gt;, DRATX &lt;int&gt;, DREM &lt;int&gt;, ENG &lt;int&gt;, FER &lt;int&gt;, GCL &lt;int&gt;, ## # GCM &lt;int&gt;, GCR &lt;int&gt;, HINS1 &lt;int&gt;, HINS2 &lt;int&gt;, HINS3 &lt;int&gt;, ## # HINS4 &lt;int&gt;, HINS5 &lt;int&gt;, HINS6 &lt;int&gt;, HINS7 &lt;int&gt;, INTP &lt;chr&gt;, ## # JWMNP &lt;chr&gt;, JWRIP &lt;chr&gt;, JWTR &lt;chr&gt;, LANX &lt;int&gt;, MAR &lt;int&gt;, ## # MARHD &lt;int&gt;, MARHM &lt;int&gt;, MARHT &lt;int&gt;, MARHW &lt;int&gt;, MARHYP &lt;int&gt;, ## # MIG &lt;int&gt;, MIL &lt;int&gt;, MLPA &lt;int&gt;, MLPB &lt;int&gt;, MLPCD &lt;int&gt;, MLPE &lt;int&gt;, ## # MLPFG &lt;int&gt;, MLPH &lt;int&gt;, MLPI &lt;int&gt;, MLPJ &lt;int&gt;, MLPK &lt;int&gt;, ## # NWAB &lt;int&gt;, NWAV &lt;int&gt;, NWLA &lt;int&gt;, NWLK &lt;int&gt;, NWRE &lt;int&gt;, OIP &lt;chr&gt;, ## # PAP &lt;chr&gt;, RELP &lt;chr&gt;, RETP &lt;chr&gt;, SCH &lt;int&gt;, SCHG &lt;chr&gt;, SCHL &lt;chr&gt;, ## # SEMP &lt;chr&gt;, SEX &lt;int&gt;, SSIP &lt;chr&gt;, SSP &lt;chr&gt;, WAGP &lt;chr&gt;, WKHP &lt;chr&gt;, ## # WKL &lt;int&gt;, WKW &lt;int&gt;, WRK &lt;int&gt;, YOEP &lt;int&gt;, ANC &lt;int&gt;, ANC1P &lt;chr&gt;, ## # ANC2P &lt;chr&gt;, DECADE &lt;int&gt;, DIS &lt;int&gt;, DRIVESP &lt;int&gt;, ESP &lt;int&gt;, ## # ESR &lt;int&gt;, FHICOVP &lt;int&gt;, FOD1P &lt;int&gt;, FOD2P &lt;int&gt;, HICOV &lt;int&gt;, ## # HISP &lt;chr&gt;, INDP &lt;chr&gt;, JWAP &lt;chr&gt;, JWDP &lt;chr&gt;, LANP &lt;int&gt;, ## # MIGPUMA &lt;chr&gt;, MIGSP &lt;chr&gt;, MSP &lt;int&gt;, NAICSP &lt;chr&gt;, NATIVITY &lt;int&gt;, ## # NOP &lt;int&gt;, OC &lt;int&gt;, OCCP &lt;chr&gt;, PAOC &lt;int&gt;, PERNP &lt;chr&gt;, PINCP &lt;chr&gt;, ## # POBP &lt;chr&gt;, POVPIP &lt;chr&gt;, POWPUMA &lt;chr&gt;, POWSP &lt;chr&gt;, PRIVCOV &lt;int&gt;, ## # PUBCOV &lt;int&gt;, QTRBIR &lt;int&gt;, ... 2.3 dplyr Using the data dictionary we can identify some interesting variables. PERSON RECORD RT 1 Record Type P .Person Record AGEP 2 Age 00 .Under 1 year 01..99 .1 to 99 years (Top-coded***) COW 1 Class of worker b .N/A (less than 16 years old/NILF who last .worked more than 5 years ago or never worked) 1 .Employee of a private for-profit company or .business, or of an individual, for wages, .salary, or commissions 2 .Employee of a private not-for-profit, .tax-exempt, or charitable organization 3 .Local government employee (city, county, etc.) 4 .State government employee 5 .Federal government employee 6 .Self-employed in own not incorporated .business, professional practice, or farm 7 .Self-employed in own incorporated .business, professional practice or farm 8 .Working without pay in family business or farm 9 .Unemployed and last worked 5 years ago or earlier or never .worked SCHL 2 Educational attainment bb .N/A (less than 3 years old) 01 .No schooling completed 02 .Nursery school, preschool 03 .Kindergarten 04 .Grade 1 05 .Grade 2 06 .Grade 3 07 .Grade 4 08 .Grade 5 09 .Grade 6 10 .Grade 7 11 .Grade 8 12 .Grade 9 13 .Grade 10 14 .Grade 11 15 .12th grade - no diploma 16 .Regular high school diploma 17 .GED or alternative credential 18 .Some college, but less than 1 year 19 .1 or more years of college credit, no degree 20 .Associate&#39;s degree 21 .Bachelor&#39;s degree 22 .Master&#39;s degree 23 .Professional degree beyond a bachelor&#39;s degree 24 .Doctorate degree WAGP 6 Wages or salary income past 12 months bbbbbb .N/A (less than 15 years old) 000000 .None 000001..999999 .$1 to 999999 (Rounded and top-coded) Note: Use ADJINC to adjust WAGP to constant dollars. WKHP 2 Usual hours worked per week past 12 months bb .N/A (less than 16 years old/did not work .during the past 12 months) 01..98 .1 to 98 usual hours 99 .99 or more usual hours WKW 1 Weeks worked during past 12 months b .N/A (less than 16 years old/did not work .during the past 12 months) 1 .50 to 52 weeks worked during past 12 months 2 .48 to 49 weeks worked during past 12 months 3 .40 to 47 weeks worked during past 12 months 4 .27 to 39 weeks worked during past 12 months 5 .14 to 26 weeks worked during past 12 months 6 .less than 14 weeks worked during past 12 months ESR 1 Employment status recode b .N/A (less than 16 years old) 1 .Civilian employed, at work 2 .Civilian employed, with a job but not at work 3 .Unemployed 4 .Armed forces, at work 5 .Armed forces, with a job but not at work 6 .Not in labor force PERNP 7 Total person&#39;s earnings bbbbbbb .N/A (less than 15 years old) 0000000 .No earnings -010000 .Loss of $10000 or more (Rounded &amp; bottom-coded .components) -000001..-009999 .Loss $1 to $9999 (Rounded components) 0000001 .$1 or break even 0000002..9999999 .$2 to $9999999 (Rounded &amp; top-coded components) Note: Use ADJINC to adjust PERNP to constant dollars. PINCP 7 Total person&#39;s income (signed) bbbbbbb .N/A (less than 15 years old) 0000000 .None -019999 .Loss of $19999 or more (Rounded &amp; bottom-coded .components) -000001..-019998 .Loss $1 to $19998 (Rounded components) 0000001 .$1 or break even 0000002..9999999 .$2 to $9999999 (Rounded &amp; top-coded components) Note: Use ADJINC to adjust PINCP to constant dollars. Let’s focus on employed civilians (ESR either 1 or 2) working full time (WKHP &gt; 32) for close to the entire year (WKW either 1 or 2). library(dplyr) # also in library(tidyverse) pop_hi &lt;- pop_hi %&gt;% filter( ESR %in% c(1, 2), as.numeric(WKHP) &gt; 32, WKW %in% c(1, 2) ) If you are unsure if a column that you want to treat as numeric contains letters, you can run the following command to get a list of the values containing letters: grep(&quot;[[:alpha:]]&quot;, pop_hi$WKHP, value = TRUE) ## character(0) We can use two functions to add new columns (or change existing ones). mutate() adds columns and keeps the previous columns transmute() adds columns and removes the previous columns This time we want to drop the columns we don’t mention. pop_hi &lt;- pop_hi %&gt;% transmute( age = as.numeric(AGEP), worker_class = factor(COW, labels = c( &quot;for-profit&quot;, &quot;not-for-profit&quot;, &quot;local government&quot;, &quot;state government&quot;, &quot;federal government&quot;, &quot;self-employed not incorporated&quot;, &quot;self-employed incorporated&quot;, &quot;family business no pay&quot; )), school = SCHL, wages = as.numeric(WAGP), top_coded_wages = WAGP == 999999 ) Creating a custom factor variable for educational attainment: education_levels &lt;- c(&quot;less than HS&quot;, &quot;HS&quot;, &quot;associates&quot;, &quot;bachelors&quot;, &quot;masters&quot;, &quot;doctorate&quot;) pop_hi$education &lt;- NA pop_hi[pop_hi$school &lt; 16,]$education &lt;- &quot;less than HS&quot; pop_hi[pop_hi$school &gt; 16 &amp; pop_hi$school &lt; 20,]$education &lt;- &quot;HS&quot; pop_hi[pop_hi$school == 20,]$education &lt;- &quot;associates&quot; pop_hi[pop_hi$school == 21,]$education &lt;- &quot;bachelors&quot; pop_hi[pop_hi$school %in% c(22, 23),]$education &lt;- &quot;masters&quot; pop_hi[pop_hi$school == 24,]$education &lt;- &quot;doctorate&quot; pop_hi &lt;- pop_hi %&gt;% mutate(education = factor(education, levels = education_levels)) 2.4 First Look at ggplot2 See the ggplot2 documentation for details and inspiration. library(ggplot2) # also in library(tidyverse) ggplot(pop_hi, aes(age, wages)) + geom_point() Income has a skewed distribution, so it is often presented/analyzed in logs. Here’s how to modify the above chart to display income in logs: pop_hi &lt;- pop_hi %&gt;% mutate(log_safe_wages = ifelse(wages == 0, 1, wages)) pop_hi %&gt;% ggplot(aes(age, log_safe_wages)) + geom_point() + scale_y_log10() 2.5 Heatmaps Heatmaps allow you to get a sense of the concetration of observations in regions where there are many overlapping points: ggplot(pop_hi, aes(age, log_safe_wages)) + geom_bin2d() + scale_y_log10() 2.6 Hexbins Hexbins use hexagons instead of squares, which helps avoid the rectangular sections in heatmaps that may misrepresent your data. You will need to install the hexbin package. install.packages(&quot;hexbin&quot;) pop_hi %&gt;% filter(wages &gt; 10000, wages &lt; 300000) %&gt;% ggplot(aes(age, wages)) + geom_hex() + scale_x_log10() Here we can see that inequality in wages for workers increases with age. 2.7 Other topics from this dataset This dataset includes information on the majors for degree holders and the industry codes. You could use that additional information to ask how well targeted majors are to particular industries and how incomes vary across choice of major. Because of the size of the data in the Hawaii sample, it would be better to ask some of these questions at the national level. Go to the ACS PUMS documentation page for more information. 2.8 Assignment Choose another pair of variables from the data dictionary and visualize them with a scatterplot, a heatmap, and a hexbin plot. "],
["facets-and-bubbles.html", "Lecture 3 Facets, Bubbles, and Transparency 3.1 Data 3.2 Facets 3.3 Bubbles 3.4 Transparency 3.5 Assignment", " Lecture 3 Facets, Bubbles, and Transparency 3.1 Data For this session, we’ll explore the Hawaii Tourism Authority (HTA) Air Seat Projection. I’ll be working with the Air Seat Projection for 2017 (revised 06/17). Feel free to download the latest available. 3.1.1 Importing non-standard Excel files The first steps in preparing a non-standard Excel file are (1) identify how many rows to skip and (2) provide column names if the column names are not neatly contained in a single row. You may also want to set the range if there is metadata at the end of the table you are importing. range overrides any skip setting, so we wont have to specify the number of rows to skip. library(readxl) seats &lt;- read_excel(&quot;data/2017 Air Seat Forecast rev 0617.xls&quot;, col_names = c( &quot;dep_city&quot;, &quot;seats2017Q1&quot;, &quot;seats2017Q2&quot;, &quot;seats2017Q3&quot;, &quot;seats2017Q4&quot;, &quot;seats2017&quot;, &quot;seats2016Q1&quot;, &quot;seats2016Q2&quot;, &quot;seats2016Q3&quot;, &quot;seats2016Q4&quot;, &quot;seats2016&quot;, &quot;seatschangeQ1&quot;, &quot;seatschangeQ2&quot;, &quot;seatschangeQ3&quot;, &quot;seatschangeQ4&quot;, &quot;seatschange&quot; ), range = &quot;A5:P78&quot;) seats ## # A tibble: 74 × 16 ## dep_city seats2017Q1 seats2017Q2 seats2017Q3 seats2017Q4 seats2017 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 TOTAL 2987920 3016376 3168233 3050112 12222641 ## 2 SCHEDULED 2966915 2996155 3140998 3029794 12133862 ## 3 CHARTERS 21005 20221 27235 20318 88779 ## 4 &lt;NA&gt; NA NA NA NA NA ## 5 US TOTAL 1996549 2108969 2215424 2071513 8392455 ## 6 SCHEDULED 1978616 2091981 2200195 2055171 8325963 ## 7 CHARTERS 17933 16988 15229 16342 66492 ## 8 &lt;NA&gt; NA NA NA NA NA ## 9 US WEST 1717254 1837080 1943653 1817441 7315428 ## 10 Anchorage 25758 15105 13674 17013 71550 ## # ... with 64 more rows, and 10 more variables: seats2016Q1 &lt;dbl&gt;, ## # seats2016Q2 &lt;dbl&gt;, seats2016Q3 &lt;dbl&gt;, seats2016Q4 &lt;dbl&gt;, ## # seats2016 &lt;dbl&gt;, seatschangeQ1 &lt;dbl&gt;, seatschangeQ2 &lt;chr&gt;, ## # seatschangeQ3 &lt;chr&gt;, seatschangeQ4 &lt;dbl&gt;, seatschange &lt;dbl&gt; Let’s add a region identifier library(dplyr) us_west_range &lt;- 10:23 us_east_range &lt;- 26:33 japan_range &lt;- 40:45 canada_range &lt;- 48:52 other_asia_range &lt;-55:58 oceania_range &lt;- 61:64 other_range &lt;- 67:74 seats$region &lt;- NA seats[us_west_range,]$region &lt;- &quot;US West&quot; seats[us_east_range,]$region &lt;- &quot;US East&quot; seats[japan_range,]$region &lt;- &quot;Japan&quot; seats[canada_range,]$region &lt;- &quot;Canada&quot; seats[other_asia_range,]$region &lt;- &quot;Other Asia&quot; seats[oceania_range,]$region &lt;- &quot;Oceania&quot; seats[other_range,]$region &lt;- &quot;Other&quot; seats &lt;- seats %&gt;% filter(!is.na(region)) seats ## # A tibble: 49 × 17 ## dep_city seats2017Q1 seats2017Q2 seats2017Q3 seats2017Q4 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Anchorage 25758 15105 13674 17013 ## 2 Bellingham 10198 318 NA 6519 ## 3 Denver 55803 51654 52585 43290 ## 4 Las Vegas 70514 74322 75839 75415 ## 5 Los Angeles 548935 647498 715338 647703 ## 6 Oakland 84571 104810 116015 90703 ## 7 Phoenix 113046 115125 125348 108863 ## 8 Portland 90207 71068 65997 81673 ## 9 Sacramento 37620 38318 38456 38456 ## 10 Salt Lake City 26370 23751 22968 28322 ## # ... with 39 more rows, and 12 more variables: seats2017 &lt;dbl&gt;, ## # seats2016Q1 &lt;dbl&gt;, seats2016Q2 &lt;dbl&gt;, seats2016Q3 &lt;dbl&gt;, ## # seats2016Q4 &lt;dbl&gt;, seats2016 &lt;dbl&gt;, seatschangeQ1 &lt;dbl&gt;, ## # seatschangeQ2 &lt;chr&gt;, seatschangeQ3 &lt;chr&gt;, seatschangeQ4 &lt;dbl&gt;, ## # seatschange &lt;dbl&gt;, region &lt;chr&gt; 3.2 Facets Let’s do a simple plot comparing 2017 seats outlook to the 2016 seats outlook. library(ggplot2) seats %&gt;% ggplot(aes(seats2016, seats2017)) + geom_point() The distribution of this data looks like a good candidate for using the log scale (high concentration in lower values and lower concentration in higher values). seats %&gt;% ggplot(aes(seats2016, seats2017)) + geom_point() + scale_x_log10() + scale_y_log10() + geom_abline(lty = 2) # dashed line type (lty) Since we have region identifiers it would be nice to divide our data and see charts of each region side-by-side. Facets allow us to make multiple charts based on a variable or set of variables. seats %&gt;% ggplot(aes(seats2016, seats2017)) + geom_point() + scale_x_log10() + scale_y_log10() + geom_abline(lty = 2) + facet_wrap(~ region) + coord_fixed() An alternative representation is to present each region using color: seats %&gt;% ggplot(aes(seats2016, seats2017, color = region, label = dep_city)) + geom_point() + scale_x_log10() + scale_y_log10() + geom_abline(lty = 2) + geom_text(check_overlap = TRUE, nudge_y = 0.1) 3.3 Bubbles Bubble charts are scatter plots (geom_point) with points that vary in size corresponding to the value of a given variable. Let’s create a measure of the size of a city’s seats relative to its regional total. seats &lt;- seats %&gt;% group_by(region) %&gt;% mutate(proportion_of_region = seats2017/sum(seats2017)) seats ## Source: local data frame [49 x 18] ## Groups: region [7] ## ## dep_city seats2017Q1 seats2017Q2 seats2017Q3 seats2017Q4 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Anchorage 25758 15105 13674 17013 ## 2 Bellingham 10198 318 NA 6519 ## 3 Denver 55803 51654 52585 43290 ## 4 Las Vegas 70514 74322 75839 75415 ## 5 Los Angeles 548935 647498 715338 647703 ## 6 Oakland 84571 104810 116015 90703 ## 7 Phoenix 113046 115125 125348 108863 ## 8 Portland 90207 71068 65997 81673 ## 9 Sacramento 37620 38318 38456 38456 ## 10 Salt Lake City 26370 23751 22968 28322 ## # ... with 39 more rows, and 13 more variables: seats2017 &lt;dbl&gt;, ## # seats2016Q1 &lt;dbl&gt;, seats2016Q2 &lt;dbl&gt;, seats2016Q3 &lt;dbl&gt;, ## # seats2016Q4 &lt;dbl&gt;, seats2016 &lt;dbl&gt;, seatschangeQ1 &lt;dbl&gt;, ## # seatschangeQ2 &lt;chr&gt;, seatschangeQ3 &lt;chr&gt;, seatschangeQ4 &lt;dbl&gt;, ## # seatschange &lt;dbl&gt;, region &lt;chr&gt;, proportion_of_region &lt;dbl&gt; Now we can modify the chart to show the importance of each city in the context of its region. seats %&gt;% filter(region %in% c(&quot;US West&quot;, &quot;US East&quot;)) %&gt;% ggplot(aes(seats2016, seats2017, color = region, label = dep_city)) + geom_abline(lty = 2) + geom_point(aes(size = proportion_of_region)) + scale_x_log10(labels = scales::comma) + scale_y_log10(labels = scales::comma) + geom_text(check_overlap = TRUE, nudge_y = 0.1) 3.4 Transparency We can also use transparency (or alpha) to make less important points less visible. We do this by setting the alpha aesthetic. Let’s try adding the alpha setting to the geom_point() call first. seats %&gt;% filter(region %in% c(&quot;US West&quot;, &quot;US East&quot;)) %&gt;% ggplot(aes(seats2016, seats2017, color = region, label = dep_city)) + geom_abline(lty = 2) + geom_point(aes(size = proportion_of_region, alpha = proportion_of_region)) + scale_x_log10(labels = scales::comma) + scale_y_log10(labels = scales::comma) + geom_text(check_overlap = TRUE, nudge_y = 0.1) Let’s add the alpha to the ggplot-level aesthetic instead, so that it also affects the text labels. seats %&gt;% filter(region %in% c(&quot;US West&quot;, &quot;US East&quot;)) %&gt;% ggplot(aes(seats2016, seats2017, color = region, label = dep_city, alpha = proportion_of_region)) + geom_abline(lty = 2) + geom_point(aes(size = proportion_of_region)) + scale_x_log10(labels = scales::comma) + scale_y_log10(labels = scales::comma) + geom_text(nudge_y = 0.1) We can combine all the regions now and use transparency to help us see how many cities are in the same area on the plot by how dark a region is. seats %&gt;% ggplot(aes(seats2016, seats2017, label = dep_city, alpha = proportion_of_region)) + geom_abline(lty = 2) + geom_point(aes(size = proportion_of_region)) + scale_x_log10(labels = scales::comma) + scale_y_log10(labels = scales::comma) + geom_text(aes(color = region), hjust = &quot;right&quot;, vjust = &quot;center&quot;) Let’s use facets so we can combine everything we’ve done so far. seats %&gt;% ggplot(aes(seats2016, seats2017, label = dep_city, alpha = proportion_of_region)) + geom_abline(lty = 2) + geom_point(aes(size = proportion_of_region), color = &quot;darkblue&quot;) + scale_x_log10(labels = scales::comma) + scale_y_log10(labels = scales::comma) + geom_text(hjust = &quot;right&quot;, vjust = &quot;center&quot;, nudge_x = -0.3) + facet_wrap(~ region) 3.5 Assignment Create a bubble plot highlighting the change in year-on-year growth rates for different quarters. Plot seatschangeQ3 on the x axis and seatschangeQ4 on the y axis. Use seats2017 to determine the size of each bubble. Facet by region. "],
["lines-and-curves.html", "Lecture 4 Lines and Curves 4.1 Data 4.2 geom_smooth 4.3 geom_abline 4.4 geom_vline 4.5 hline", " Lecture 4 Lines and Curves 4.1 Data We will use data on the number of active duty personnel in Hawaii. The first dataset is an Excel file pulled from the State of Hawaii Department of Business, Economic Development, and Tourism (DBEDT) 2015 State of Hawaii Data Book. See the line listed as, “10.03 - Active Duty Personnel, by Service: 1953 to 2015.” The data is originally from the US Defense Manpower Data Center library(readxl) library(dplyr) library(magrittr) mil_personnel &lt;- read_excel(&quot;data/100315.xls&quot;, range = &quot;A5:L38&quot;, col_types = &quot;numeric&quot;) mil_personnel &lt;- bind_rows( select(mil_personnel, 1:6) %&gt;% set_colnames(c(&quot;Year&quot;, &quot;Total&quot;, &quot;Army&quot;, &quot;Navy&quot;, &quot;Marine Corps&quot;, &quot;Air Force&quot;)), select(mil_personnel, 7:12) %&gt;% set_colnames(c(&quot;Year&quot;, &quot;Total&quot;, &quot;Army&quot;, &quot;Navy&quot;, &quot;Marine Corps&quot;, &quot;Air Force&quot;)) ) %&gt;% filter(!is.na(Year)) mil_personnel ## # A tibble: 62 × 6 ## Year Total Army Navy `Marine Corps` `Air Force` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1953 24785 5872 7657 6040 5216 ## 2 1954 23654 7957 6443 4155 5099 ## 3 1955 40258 19821 5211 9677 5549 ## 4 1956 37470 16531 5237 9490 6212 ## 5 1957 40683 17511 5466 9608 8098 ## 6 1958 35076 14672 4908 8670 6826 ## 7 1959 36310 15438 5309 8470 7093 ## 8 1960 35412 15492 5687 7756 6477 ## 9 1961 39474 16945 5774 9679 7076 ## 10 1962 41657 17645 6664 9903 7445 ## # ... with 52 more rows 4.2 geom_smooth geom_smooth allows you to have smooth lines appear in your chart. With no argument, it will choose loess for series shorter than 1,000 observations. It shows a shaded confidence interval. library(ggplot2) mil_personnel %&gt;% ggplot(aes(Year, Total)) + geom_point() + geom_smooth() Here’s what it looks like if we fit a linear model instead: mil_personnel %&gt;% ggplot(aes(Year, Total)) + geom_point() + geom_smooth(method = &quot;lm&quot;) We can also just have a line chart that connects the points: mil_personnel %&gt;% ggplot(aes(Year, Total)) + geom_point() + geom_line() 4.3 geom_abline geom_abline allows you to display lines with a specific intercept and slope. If no intercept or slope is provided, a 45-degree line will be shown. x = rnorm(100) y = 2.5 + 1.2 * x + rnorm(100) test_data &lt;- data_frame(x, y) test_data %&gt;% ggplot(aes(x, y)) + geom_point() + xlim(-2, 6) + ylim(-2, 6) + coord_fixed() + geom_abline() test_data %&gt;% ggplot(aes(x, y)) + geom_point() + xlim(-2, 6) + ylim(-2, 6) + coord_fixed() + geom_abline() + geom_abline(intercept = 2.5, slope = 1.2, color = &quot;red&quot;) 4.4 geom_vline geom_vline allows you to draw vertical lines by specifying an x intercept. test_data %&gt;% ggplot(aes(x, y)) + geom_point() + xlim(-2, 6) + ylim(-2, 6) + coord_fixed() + geom_abline() + geom_abline(intercept = 2.5, slope = 1.2, color = &quot;red&quot;) + geom_vline(xintercept = 2, color = &quot;blue&quot;) 4.5 hline geom_vline allows you to draw vertical lines by specifying an x intercept. test_data %&gt;% ggplot(aes(x, y)) + geom_point() + xlim(-2, 6) + ylim(-2, 6) + coord_fixed() + geom_abline() + geom_abline(intercept = 2.5, slope = 1.2, color = &quot;red&quot;) + geom_vline(xintercept = 2, color = &quot;blue&quot;) + geom_hline(yintercept = 1, color = &quot;#4FCC53&quot;, lty = 2) "],
["ggplot-exts.html", "Lecture 5 ggplot2 Extensions 5.1 Data 5.2 ggplot2 extensions 5.3 ggjoy 5.4 scatterplot matrix (GGally::ggscatmat)", " Lecture 5 ggplot2 Extensions 5.1 Data For this section, we’ll look at data from the American Community Survey (ACS) on immigration. To download the data, Go to the American FactFinder website. Click on the “Download Center” section, then click the “DOWNLOAD CENTER” button. Click the “NEXT” button, since we know the table we want to download. Select “American Community Survey” from the Program dropdown. Select “2015 ACS 5-year estimates”, click the “ADD TO YOUR SELECTIONS” button, then click “NEXT” Select “County - 050” from the geographic type dropdown, then select “All Counties within United States”, click the “ADD TO YOUR SELECTIONS” button, then click “NEXT” Type income mobility in the “topic or table name”&quot; search box, then select the option that reads: “B07011: MEDIAN INCOME IN THE PAST 12 MONTHS (IN 2015 INFLATION-ADJUSTED DOLLARS) BY GEOGRAPHICAL MOBILITY IN THE PAST YEAR FOR CURRENT RESIDENCE IN THE UNITED STATES” Click “GO”, then check the checkbox beside the table we found. Now click on the “Download” button, and uncheck the option that says, “Include descriptive data element names.” Click “Ok” to create your zip file. Once the file has been created, click “DOWNLOAD” to download the zip file. The following will assume you moved the following files within the zip to your data folder: ACS_15_5YR_B07011_with_ann.csv ACS_15_5YR_B07011_metadata.csv The file ACS_15_5YR_B07011.txt tells us how to interpret codes within our data. It is possible for median values to be followed by a + or - if they are in the upper or lower open-ended interval. In our dataset we don’t have any medians in the upper open-ended interval, but we do have entries in the lower open-ended interval. library(readr) acs &lt;- read_csv(&quot;data/ACS_15_5YR_B07011_with_ann.csv&quot;, col_types = strrep(&quot;c&quot;, 15), na = c(&quot;-&quot;, &quot;(X)&quot;)) meta &lt;- read_csv(&quot;data/ACS_15_5YR_B07011_metadata.csv&quot;) meta ## # A tibble: 14 × 2 ## GEO.id ## &lt;chr&gt; ## 1 GEO.id2 ## 2 GEO.display-label ## 3 HD01_VD02 ## 4 HD02_VD02 ## 5 HD01_VD03 ## 6 HD02_VD03 ## 7 HD01_VD04 ## 8 HD02_VD04 ## 9 HD01_VD05 ## 10 HD02_VD05 ## 11 HD01_VD06 ## 12 HD02_VD06 ## 13 HD01_VD07 ## 14 HD02_VD07 ## # ... with 1 more variables: Id &lt;chr&gt; Let’s keep only the variables we care about, using more informative variable names. library(dplyr) acs_mobility &lt;- acs %&gt;% transmute( geo = `GEO.display-label`, same_house = HD01_VD03, same_county = HD01_VD04, same_state = HD01_VD05, same_country = HD01_VD06, different_country = HD01_VD07 ) acs_mobility ## # A tibble: 1,949 × 6 ## geo same_house same_county same_state same_country ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Autauga County, Alabama 27553 18655 27643 35870 ## 2 Barbour County, Alabama 17263 17363 8165 12667 ## 3 Bibb County, Alabama 21489 16112 8804 &lt;NA&gt; ## 4 Butler County, Alabama 19499 11734 20769 23887 ## 5 Chambers County, Alabama 20708 14522 21218 18516 ## 6 Chilton County, Alabama 23668 20646 15739 33464 ## 7 Clay County, Alabama 19201 18836 20596 11350 ## 8 Cleburne County, Alabama 21888 11583 20380 16475 ## 9 Coffee County, Alabama 24325 18957 11906 31702 ## 10 Colbert County, Alabama 22419 18389 17330 20093 ## # ... with 1,939 more rows, and 1 more variables: different_country &lt;chr&gt; Now we can add an indicator for whether the median value is in the lowest available interval. This would mean that the median value presented has been bottom-coded. acs_mobility &lt;- acs_mobility %&gt;% mutate( same_country_bc = grepl(&quot;[0-9]*-&quot;, same_country), different_country_bc = grepl(&quot;[0-9]*-&quot;, different_country) ) acs_mobility ## # A tibble: 1,949 × 8 ## geo same_house same_county same_state same_country ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Autauga County, Alabama 27553 18655 27643 35870 ## 2 Barbour County, Alabama 17263 17363 8165 12667 ## 3 Bibb County, Alabama 21489 16112 8804 &lt;NA&gt; ## 4 Butler County, Alabama 19499 11734 20769 23887 ## 5 Chambers County, Alabama 20708 14522 21218 18516 ## 6 Chilton County, Alabama 23668 20646 15739 33464 ## 7 Clay County, Alabama 19201 18836 20596 11350 ## 8 Cleburne County, Alabama 21888 11583 20380 16475 ## 9 Coffee County, Alabama 24325 18957 11906 31702 ## 10 Colbert County, Alabama 22419 18389 17330 20093 ## # ... with 1,939 more rows, and 3 more variables: different_country &lt;chr&gt;, ## # same_country_bc &lt;lgl&gt;, different_country_bc &lt;lgl&gt; Let’s see how many counties have observations that are bottom coded: acs_mobility %&gt;% summarize(same_country_bc = sum(same_country_bc), different_country_bc = sum(different_country_bc), counties = n()) ## # A tibble: 1 × 3 ## same_country_bc different_country_bc counties ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 15 64 1949 Let’s see what the typical bottom-coded values are: acs_mobility %&gt;% filter(same_country_bc) %&gt;% select(same_country) %&gt;% table() ## . ## 2,500- ## 15 acs_mobility %&gt;% filter(different_country_bc) %&gt;% select(different_country) %&gt;% table() ## . ## 2,500- ## 64 In both cases the bottom-coded interval is the range from zero to 2,500. Since this is a small number of counties given the entire range, let’s simply set the bottom-coded values to equal the upper-bound of their interval (i.e., 2,500). acs_mobility &lt;- acs_mobility %&gt;% transmute( geo = geo, same_house = as.integer(same_house), same_county = as.integer(same_county), same_state = as.integer(same_state), same_country = if_else(same_country_bc, 2500L, as.integer(same_country)), different_country = if_else(different_country_bc, 2500L, as.integer(different_country)) ) acs_mobility ## # A tibble: 1,949 × 6 ## geo same_house same_county same_state same_country ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Autauga County, Alabama 27553 18655 27643 35870 ## 2 Barbour County, Alabama 17263 17363 8165 12667 ## 3 Bibb County, Alabama 21489 16112 8804 NA ## 4 Butler County, Alabama 19499 11734 20769 23887 ## 5 Chambers County, Alabama 20708 14522 21218 18516 ## 6 Chilton County, Alabama 23668 20646 15739 33464 ## 7 Clay County, Alabama 19201 18836 20596 11350 ## 8 Cleburne County, Alabama 21888 11583 20380 16475 ## 9 Coffee County, Alabama 24325 18957 11906 31702 ## 10 Colbert County, Alabama 22419 18389 17330 20093 ## # ... with 1,939 more rows, and 1 more variables: different_country &lt;int&gt; Let’s rearrange the data into tidy format (one observation per row). library(tidyr) tidy_acs &lt;- acs_mobility %&gt;% gather(location_last_year, median_income, -geo, factor_key = TRUE) tidy_acs ## # A tibble: 9,745 × 3 ## geo location_last_year median_income ## &lt;chr&gt; &lt;fctr&gt; &lt;int&gt; ## 1 Autauga County, Alabama same_house 27553 ## 2 Barbour County, Alabama same_house 17263 ## 3 Bibb County, Alabama same_house 21489 ## 4 Butler County, Alabama same_house 19499 ## 5 Chambers County, Alabama same_house 20708 ## 6 Chilton County, Alabama same_house 23668 ## 7 Clay County, Alabama same_house 19201 ## 8 Cleburne County, Alabama same_house 21888 ## 9 Coffee County, Alabama same_house 24325 ## 10 Colbert County, Alabama same_house 22419 ## # ... with 9,735 more rows 5.2 ggplot2 extensions There are many extensions the community have made that build on ggplot2. The following link provides a gallery of many of these extensions: ggplot2 extensions Some others that are usefull are ggjoy and GGally. 5.3 ggjoy Make sure ggjoy is installed. install.packages(&quot;ggjoy&quot;) ggjoy gives us the ability to stack kernel density plots. library(ggjoy) ggplot(tidy_acs, aes(x = median_income, y = location_last_year, group(location_last_year))) + geom_joy() + theme_joy() This plot shows us that, on average, the distance moved in the past year is inversely related to median income. 5.4 scatterplot matrix (GGally::ggscatmat) Make sure you have GGally installed. install.packages(&quot;GGally&quot;) One particular library, GGally, has a great set of visualizations to extend those that come prebuilt with ggplot. One common visualization tool that is missing from ggplot is the scatterplot matrix. While base R provides splom() in the lattice library, GGally::ggpairs and GGally::ggscatmat pr ovide an easy tool to create a scatterplot matrix with ggplot2. library(GGally) acs_mobility %&gt;% as.data.frame() %&gt;% ggscatmat(columns = 2:ncol(.), alpha = 0.1) "],
["boxplots-and-violins.html", "Lecture 6 Boxplots and Violin Plots 6.1 Data 6.2 Boxplots 6.3 Violin Plots 6.4 Dot Plots 6.5 Assignment", " Lecture 6 Boxplots and Violin Plots Boxplots and violin plots are two important tools for visualizing the distribution of data within a dataset. The boxplot highlights the median, key percentiles, and outliers within a dataset. The violin plot takes a kernel density plot, rotates it 90 degrees, then mirrors it about the axis to create a shape that sometimes resembles a violin. 6.1 Data The Social Security Administration releases data on earnings and employment each year. We’ll take a look at the data for 2014: https://www.ssa.gov/policy/docs/statcomps/eedata_sc/2014/index.html We’re going to download Table 1: “Number of persons with Social Security (OASDI) taxable earnings, amount taxable, and contributions, by state or other area, sex, and type of earnings, 2014” Save that file as ‘ssa_earnings.xlsx’ in the data folder library(readxl) ssa &lt;- read_xlsx(&quot;data/ssa_earnings.xlsx&quot;, range = &quot;A7:J159&quot;, col_names = c(&quot;state&quot;, &quot;gender&quot;, &quot;other&quot;, &quot;other2&quot;, &quot;number.total&quot;, &quot;number.wage&quot;, &quot;number.self&quot;, &quot;earnings.total&quot;, &quot;earnings.wage&quot;, &quot;earnings.self&quot;)) ssa ## # A tibble: 153 × 10 ## state gender other other2 number.total number.wage number.self ## &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama &lt;NA&gt; NA NA 2355477 2215535 255253 ## 2 &lt;NA&gt; Men NA NA 1200468 1116458 138895 ## 3 &lt;NA&gt; Women NA NA 1155009 1099077 116357 ## 4 Alaska &lt;NA&gt; NA NA 400007 375833 47696 ## 5 &lt;NA&gt; Men NA NA 223464 209694 27884 ## 6 &lt;NA&gt; Women NA NA 176543 166140 19812 ## 7 Arizona &lt;NA&gt; NA NA 3189785 2997567 334292 ## 8 &lt;NA&gt; Men NA NA 1660088 1551488 185753 ## 9 &lt;NA&gt; Women NA NA 1529697 1446079 148539 ## 10 Arkansas &lt;NA&gt; NA NA 1468898 1376249 163320 ## # ... with 143 more rows, and 3 more variables: earnings.total &lt;dbl&gt;, ## # earnings.wage &lt;dbl&gt;, earnings.self &lt;dbl&gt; The starting format is far from ideal. Each row should represent one group, so we don’t need any of the rows with totals. It’s important to always read any footnotes and documentation that comes with the data you plan to use. Footnote c for this table indicates that individuals with both wage and salary employment will be counted in both groups, but only once in the total. It is important to be aware of this double counting. library(tidyr) library(dplyr) ssa_long &lt;- ssa %&gt;% fill(state) %&gt;% filter(!is.na(gender)) %&gt;% reshape(varying = 5:10, direction = &quot;long&quot;, timevar = &quot;earnings_type&quot;) %&gt;% select(state, gender, earnings_type, number, earnings) %&gt;% mutate(per_capita = earnings / number) 6.2 Boxplots library(ggplot2) ssa_long %&gt;% filter(earnings_type != &quot;total&quot;) %&gt;% ggplot(aes(gender, per_capita)) + geom_boxplot() ssa_long %&gt;% ggplot(aes(gender, per_capita, fill = gender)) + geom_boxplot() + facet_grid(~ earnings_type) 6.3 Violin Plots Let’s repeat the above plots using the violin plot type. ssa_long %&gt;% filter(earnings_type != &quot;total&quot;) %&gt;% ggplot(aes(gender, per_capita)) + geom_violin() ssa_long %&gt;% ggplot(aes(gender, per_capita, color = gender, fill = gender)) + geom_violin() + facet_grid(~ earnings_type) 6.4 Dot Plots Dot plots appear similar to violin plots, but dot plots may be easier to interpret: ssa_long %&gt;% ggplot(aes(gender, per_capita, color = gender, fill = gender)) + geom_dotplot(binaxis = &quot;y&quot;, stackdir = &quot;center&quot;, position = &quot;dodge&quot;) + facet_grid(~ earnings_type) 6.5 Assignment Create your own visualization of the SSA data using one of the above distribution visualizations. "],
["geom-spoke.html", "Lecture 7 Spatial Visualizations 7.1 Data 7.2 geom_spoke 7.3 maps 7.4 gganimate 7.5 glyphs", " Lecture 7 Spatial Visualizations 7.1 Data The data for this class will come from the National Oceanic and Atmospheric Administration (NOAA) U.S. Wind Climatology datasets (https://www.ncdc.noaa.gov/societal-impacts/wind/). Download the files for both the u-component and the v-component of the wind data. To open these files in R, we’ll need to install the ncdf4 package, which provides an interface to Unidata’s netCDF data file format: install.packages(c(&quot;ncdf4&quot;, &quot;ncdf4.helpers&quot;, &quot;PCICt&quot;)) Let’s load up the u-component file first: library(ncdf4) uwnd_nc &lt;- nc_open(&quot;data/uwnd.sig995.2017.nc&quot;) uwnd_nc ## File data/uwnd.sig995.2017.nc (NC_FORMAT_NETCDF4_CLASSIC): ## ## 2 variables (excluding dimension variables): ## float uwnd[lon,lat,time] ## long_name: mean Daily u-wind at sigma level 995 ## units: m/s ## precision: 2 ## least_significant_digit: 1 ## GRIB_id: 33 ## GRIB_name: UGRD ## var_desc: u-wind ## dataset: NCEP Reanalysis Daily Averages ## level_desc: Surface ## statistic: Mean ## parent_stat: Individual Obs ## missing_value: -9.96920996838687e+36 ## valid_range: -102.199996948242 ## valid_range: 102.199996948242 ## actual_range: -26.9250011444092 ## actual_range: 29.8999996185303 ## double time_bnds[nbnds,time] ## ## 4 dimensions: ## lat Size:73 ## units: degrees_north ## actual_range: 90 ## actual_range: -90 ## long_name: Latitude ## standard_name: latitude ## axis: Y ## lon Size:144 ## units: degrees_east ## long_name: Longitude ## actual_range: 0 ## actual_range: 357.5 ## standard_name: longitude ## axis: X ## time Size:198 *** is unlimited *** ## long_name: Time ## delta_t: 0000-00-01 00:00:00 ## standard_name: time ## axis: T ## units: hours since 1800-01-01 00:00:0.0 ## avg_period: 0000-00-01 00:00:00 ## coordinate_defines: start ## actual_range: 1902192 ## actual_range: 1906920 ## nbnds Size:2 ## ## 7 global attributes: ## Conventions: COARDS ## title: mean daily NMC reanalysis (2014) ## history: created 2013/12 by Hoop (netCDF2.3) ## description: Data is from NMC initialized reanalysis ## (4x/day). These are the 0.9950 sigma level values. ## platform: Model ## References: http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html ## dataset_title: NCEP-NCAR Reanalysis 1 Let’s store the uwnd observations in the netCDF file for the u-component: ## packages for getting nice time variables from the netCDF file library(ncdf4.helpers) library(PCICt) ## general data munging packages library(dplyr) library(tibble) uwnd &lt;- ncvar_get(uwnd_nc, &quot;uwnd&quot;) uwnd_time &lt;- nc.get.time.series(uwnd_nc, v = &quot;uwnd&quot;, time.dim.name = &quot;time&quot;) uwnd_lon &lt;- ncvar_get(uwnd_nc, &quot;lon&quot;) uwnd_lat &lt;- ncvar_get(uwnd_nc, &quot;lat&quot;) nc_close(uwnd_nc) uwnd_df &lt;- uwnd %&gt;% as.data.frame.table(responseName = &quot;uwnd&quot;, stringsAsFactors = FALSE) %&gt;% rename(a = Var1, b = Var2, c = Var3) %&gt;% cbind.data.frame(expand.grid(uwnd_lon, uwnd_lat, uwnd_time)) %&gt;% rename(lon = Var1, lat = Var2, time = Var3) %&gt;% select(lon, lat, time, uwnd) %&gt;% as.tibble() uwnd_df ## # A tibble: 2,081,376 × 4 ## lon lat time uwnd ## &lt;dbl&gt; &lt;dbl&gt; &lt;S3: PCICt&gt; &lt;dbl&gt; ## 1 0.0 90 2017-01-01 -2.29999971 ## 2 2.5 90 2017-01-01 -1.99999964 ## 3 5.0 90 2017-01-01 -1.69999957 ## 4 7.5 90 2017-01-01 -1.34999967 ## 5 10.0 90 2017-01-01 -1.02499962 ## 6 12.5 90 2017-01-01 -0.72499961 ## 7 15.0 90 2017-01-01 -0.39999962 ## 8 17.5 90 2017-01-01 -0.04999962 ## 9 20.0 90 2017-01-01 0.27500039 ## 10 22.5 90 2017-01-01 0.60000038 ## # ... with 2,081,366 more rows Now we need to do the same for the v-component of the wind vectors. Since we know the lat, lon, and time dimensions are repeated, we can join directly to the previous data.frame: vwnd_nc &lt;- nc_open(&quot;data/vwnd.sig995.2017.nc&quot;) vwnd &lt;- ncvar_get(vwnd_nc, &quot;vwnd&quot;) vwnd_time &lt;- nc.get.time.series(vwnd_nc, v = &quot;vwnd&quot;, time.dim.name = &quot;time&quot;) vwnd_lon &lt;- ncvar_get(vwnd_nc, &quot;lon&quot;) vwnd_lat &lt;- ncvar_get(vwnd_nc, &quot;lat&quot;) nc_close(vwnd_nc) wind &lt;- vwnd %&gt;% as.data.frame.table(responseName = &quot;vwnd&quot;, stringsAsFactors = FALSE) %&gt;% cbind.data.frame(uwnd_df) %&gt;% rename(lon2 = Var1, lat2 = Var2, time2 = Var3) %&gt;% select(lon, lat, time, vwnd, uwnd) %&gt;% as.tibble() wind ## # A tibble: 2,081,376 × 5 ## lon lat time vwnd uwnd ## &lt;dbl&gt; &lt;dbl&gt; &lt;S3: PCICt&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0 90 2017-01-01 7.150002 -2.29999971 ## 2 2.5 90 2017-01-01 7.250002 -1.99999964 ## 3 5.0 90 2017-01-01 7.350002 -1.69999957 ## 4 7.5 90 2017-01-01 7.375001 -1.34999967 ## 5 10.0 90 2017-01-01 7.475002 -1.02499962 ## 6 12.5 90 2017-01-01 7.475002 -0.72499961 ## 7 15.0 90 2017-01-01 7.525002 -0.39999962 ## 8 17.5 90 2017-01-01 7.550002 -0.04999962 ## 9 20.0 90 2017-01-01 7.550002 0.27500039 ## 10 22.5 90 2017-01-01 7.525002 0.60000038 ## # ... with 2,081,366 more rows Otherwise, we would need to merge these data.frames to get uwnd and vwnd together with the following, which takes long time to run: wind &lt;- merge(uwnd_df, vwnd_df) 7.2 geom_spoke To represent these wind vectors we’ll use the geom_spoke(). We’ll start just plotting wind patterns for January 1, 2017: library(ggplot2) wind &lt;- wind %&gt;% mutate(angle = atan2(vwnd, uwnd), radius = sqrt(uwnd^2 + vwnd^2), time = as.POSIXct(time)) wind %&gt;% filter(time == as.POSIXct(&quot;2017-01-01&quot;, tz = &quot;GMT&quot;)) %&gt;% ggplot(aes(lon, lat)) + geom_spoke(aes(angle = angle, radius = radius, alpha = radius, color = angle)) + scale_color_gradient2(low = &quot;#132B43&quot;, mid = &quot;#56B1F7&quot;, high = &quot;#132B43&quot;) 7.3 maps install.packages(&quot;maps&quot;) Map data will help to provide some context to this wind figure. We’ll use geom_polygon to plot the world centered on the Pacific Ocean (world2) using the map_data() function. world &lt;- map_data(&quot;world2&quot;) wind %&gt;% filter(time == as.POSIXct(&quot;2017-01-01&quot;, tz = &quot;GMT&quot;)) %&gt;% ggplot(aes(lon, lat)) + geom_polygon(data = world, aes(x=long, y = lat, group = group), color = &quot;green&quot;, fill = NA) + coord_fixed(1) + geom_spoke(aes(angle = angle, radius = radius, alpha = radius, color = angle)) + scale_color_gradient2(low = &quot;#132B43&quot;, mid = &quot;#56B1F7&quot;, high = &quot;#132B43&quot;) + theme_minimal() 7.4 gganimate The gganimate package lets us animate the above chart. If you want to be able to save animations as an mp4, you will need install ffmpeg (https://www.ffmpeg.org/download.html). You can install gganimate with devtools: devtools::install_github(&quot;dgrtwo/gganimate&quot;) library(gganimate) f &lt;- wind %&gt;% ggplot(aes(lon, lat)) + geom_polygon(data = world, aes(x=long, y = lat, group = group), color = &quot;green&quot;, fill = NA) + coord_fixed(1) + geom_spoke(aes(angle = angle, radius = radius, alpha = radius, color = angle, frame = time)) + scale_color_gradient2(low = &quot;#132B43&quot;, mid = &quot;#56B1F7&quot;, high = &quot;#132B43&quot;) + theme_minimal() gganimate(f) 7.5 glyphs glyphs provide another useful way of analyzing spatial data with a time dimesion. This shows a tiny line charts representing the north-south component of the wind at each longitude/latitude combination. library(GGally) wind$day &lt;- as.numeric(julian(wind$time, as.POSIXct(&quot;2017-01-01&quot;, tz = &quot;GMT&quot;))) wind$day_flip &lt;- -wind$day vwnd_gly &lt;- glyphs(wind, &quot;lon&quot;, &quot;day&quot;, &quot;lat&quot;, &quot;vwnd&quot;, height=2.5) uwnd_gly &lt;- glyphs(wind, &quot;lon&quot;, &quot;day&quot;, &quot;lat&quot;, &quot;uwnd&quot;, height=2.5) ggplot(vwnd_gly, aes(gx, gy, group = gid)) + add_ref_lines(vwnd_gly, color = &quot;grey90&quot;) + add_ref_boxes(vwnd_gly, color = &quot;grey90&quot;) + geom_path() + theme_bw() + labs(x = &quot;&quot;, y = &quot;&quot;) Let’s focus in on just the continental US: usa &lt;- map_data(&quot;usa&quot;) usa_long_range &lt;- range(usa$long) usa_lat_range &lt;- range(usa$lat) usa_wind &lt;- wind %&gt;% filter(lon &gt;= (usa_long_range[1] %% 360) &amp; lon &lt;= (usa_long_range[2] %% 360) &amp; lat &gt;= usa_lat_range[1] &amp; lat &lt;= usa_lat_range[2]) usa_wind$day &lt;- as.numeric(julian(usa_wind$time, as.POSIXct(&quot;2017-01-01&quot;, tz = &quot;GMT&quot;))) usa_wind$day_flip &lt;- -usa_wind$day usa_vwnd_gly &lt;- glyphs(usa_wind, &quot;lon&quot;, &quot;day&quot;, &quot;lat&quot;, &quot;vwnd&quot;, height=2.5) usa_uwnd_gly &lt;- glyphs(usa_wind, &quot;lon&quot;, &quot;uwnd&quot;, &quot;lat&quot;, &quot;day_flip&quot;, height=2.5) ggplot(usa_vwnd_gly, aes(gx, gy, group = gid)) + geom_polygon(data = usa, aes(x = long %% 360, y = lat %% 360, group = group), fill = &quot;grey60&quot;) + add_ref_lines(usa_vwnd_gly, color = &quot;grey90&quot;) + add_ref_boxes(usa_vwnd_gly, color = &quot;grey90&quot;) + geom_path(alpha = 0.9) + theme_bw() + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;North-South&quot;) ggplot(usa_uwnd_gly, aes(gx, gy, group = gid)) + geom_polygon(data = usa, aes(x = long %% 360, y = lat %% 360, group = group), fill = &quot;grey60&quot;) + add_ref_lines(usa_uwnd_gly, color = &quot;grey90&quot;) + add_ref_boxes(usa_uwnd_gly, color = &quot;grey90&quot;) + geom_path(alpha = 0.9) + theme_bw() + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;East-West&quot;) "],
["area-and-ribbons.html", "Lecture 8 geom_area and geom_ribbon 8.1 Data 8.2 geom_area 8.3 geom_ribbon", " Lecture 8 geom_area and geom_ribbon 8.1 Data The US Bureau of Labor Statistics (BLS) conducts the American Time Use Survey (ATUS). You can download the text form of the ATUS by going to the BLS data page, finding the section labelled Spending &amp; Time Use, then clicking on the “Text Files” button on the row for the ATUS. Or by using the following link: https://download.bls.gov/pub/time.series/tu/ 8.1.1 Downloading a file from the internet While you can manually download the files from the above URL, download.file() lets you download files from within R. The first argument is the URL of the resource you want to download. The second argument is the destination for the file. The following requests will require you to create the tu folder. download.file(&quot;https://download.bls.gov/pub/time.series/tu/tu.txt&quot;, &quot;data/tu/tu.txt&quot;) download.file(&quot;https://download.bls.gov/pub/time.series/tu/tu.series&quot;, &quot;data/tu/tu.series&quot;) download.file(&quot;https://download.bls.gov/pub/time.series/tu/tu.data.0.Current&quot;, &quot;data/tu/tu.data.0.Current&quot;) The file tu.txt contains the documentation for the time use (tu) survey data. Section 2 of that file provides descriptions of each of the files in the pub/time.series/tu folder. From that list we can see that tu.series will give us a list of the available series. library(readr) series_defn &lt;- read_tsv(&quot;data/tu/tu.series&quot;) series_defn ## # A tibble: 85,277 × 43 ## series_id seasonal stattype_code datays_code sex_code ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 TUU10100AA01000007 U 10100 01 0 ## 2 TUU10100AA01000013 U 10100 01 0 ## 3 TUU10100AA01000014 U 10100 01 0 ## 4 TUU10100AA01000015 U 10100 01 0 ## 5 TUU10100AA01000018 U 10100 01 0 ## 6 TUU10100AA01000019 U 10100 01 0 ## 7 TUU10100AA01000025 U 10100 01 0 ## 8 TUU10100AA01000035 U 10100 01 0 ## 9 TUU10100AA01000036 U 10100 01 0 ## 10 TUU10100AA01000037 U 10100 01 0 ## # ... with 85,267 more rows, and 38 more variables: region_code &lt;chr&gt;, ## # lfstat_code &lt;chr&gt;, educ_code &lt;chr&gt;, maritlstat_code &lt;chr&gt;, ## # age_code &lt;chr&gt;, orig_code &lt;chr&gt;, race_code &lt;chr&gt;, mjcow_code &lt;chr&gt;, ## # nmet_code &lt;int&gt;, where_code &lt;chr&gt;, sjmj_code &lt;int&gt;, ## # timeday_code &lt;chr&gt;, actcode_code &lt;chr&gt;, industry_code &lt;chr&gt;, ## # occ_code &lt;chr&gt;, prhhchild_code &lt;chr&gt;, earn_code &lt;chr&gt;, ## # disability_code &lt;chr&gt;, who_code &lt;chr&gt;, hhnscc03_code &lt;chr&gt;, ## # schenr_code &lt;int&gt;, prownhhchild_code &lt;chr&gt;, work_code &lt;int&gt;, ## # elnum_code &lt;chr&gt;, ecage_code &lt;chr&gt;, elfreq_code &lt;int&gt;, ## # eldur_code &lt;chr&gt;, elwho_code &lt;chr&gt;, ecytd_code &lt;int&gt;, ## # elder_code &lt;int&gt;, lfstatw_code &lt;chr&gt;, pertype_code &lt;chr&gt;, ## # series_title &lt;chr&gt;, footnote_codes &lt;chr&gt;, begin_year &lt;int&gt;, ## # begin_period &lt;chr&gt;, end_year &lt;int&gt;, end_period &lt;chr&gt; There is a lot here to process. The columns we care most about for now are series_id and series_title. Using select() from the dplyr library, we can show just the columns we care about. library(dplyr) series_defn %&gt;% select(series_id, series_title) ## # A tibble: 85,277 × 2 ## series_id ## &lt;chr&gt; ## 1 TUU10100AA01000007 ## 2 TUU10100AA01000013 ## 3 TUU10100AA01000014 ## 4 TUU10100AA01000015 ## 5 TUU10100AA01000018 ## 6 TUU10100AA01000019 ## 7 TUU10100AA01000025 ## 8 TUU10100AA01000035 ## 9 TUU10100AA01000036 ## 10 TUU10100AA01000037 ## # ... with 85,267 more rows, and 1 more variables: series_title &lt;chr&gt; 8.1.2 Pairing down the list of variables Let’s look for variables on sleep, work, and leisure: series_defn %&gt;% select(series_title) %&gt;% filter(grepl(&quot;sleep&quot;, series_title, ignore.case = TRUE)) ## # A tibble: 1,310 × 1 ## series_title ## &lt;chr&gt; ## 1 Avg hrs per day - Sleeping ## 2 Avg hrs per day - Sleeping, Weekend days and holidays ## 3 Avg hrs per day - Sleeping, Nonholiday weekdays ## 4 Avg hrs per day - Sleeping, Employed ## 5 Avg hrs per day - Sleeping, Weekend days and holidays, Employed ## 6 Avg hrs per day - Sleeping, Nonholiday weekdays, Employed ## 7 Avg hrs per day - Sleeping, Employed, on days worked ## 8 Avg hrs per day - Sleeping, Weekend days and holidays, Employed, on days wo ## 9 Avg hrs per day - Sleeping, Nonholiday weekdays, Employed, on days worked ## 10 Avg hrs per day - Sleeping, Employed full time ## # ... with 1,300 more rows Since this simple search returns a ton of results, let’s further filter by ‘employed’ and ‘per day’: series_defn %&gt;% select(series_title) %&gt;% filter(grepl(&quot;per day.*sleep.*employed&quot;, series_title, ignore.case = TRUE)) ## # A tibble: 154 × 1 ## series_title ## &lt;chr&gt; ## 1 Avg hrs per day - Sleeping, Employed ## 2 Avg hrs per day - Sleeping, Weekend days and holidays, Employed ## 3 Avg hrs per day - Sleeping, Nonholiday weekdays, Employed ## 4 Avg hrs per day - Sleeping, Employed, on days worked ## 5 Avg hrs per day - Sleeping, Weekend days and holidays, Employed, on days wo ## 6 Avg hrs per day - Sleeping, Nonholiday weekdays, Employed, on days worked ## 7 Avg hrs per day - Sleeping, Employed full time ## 8 Avg hrs per day - Sleeping, Weekend days and holidays, Employed full time ## 9 Avg hrs per day - Sleeping, Nonholiday weekdays, Employed full time ## 10 Avg hrs per day - Sleeping, Employed full time, on days worked ## # ... with 144 more rows Now let’s filter further by ‘employed full time’, ‘nonholiday weekdays’, and ‘on days worked’: series_defn %&gt;% select(series_title) %&gt;% filter(grepl(&quot;per day.*sleep.*nonholiday weekdays.*employed full time.*on days worked&quot;, series_title, ignore.case = TRUE)) ## # A tibble: 6 × 1 ## series_title ## &lt;chr&gt; ## 1 Avg hrs per day - Sleeping, Nonholiday weekdays, Employed full time, on day ## 2 Avg hrs per day - Sleeping, Nonholiday weekdays, Employed full time, on day ## 3 Avg hrs per day - Sleeping, Nonholiday weekdays, Employed full time, on day ## 4 Avg hrs per day for participants - Sleeping, Nonholiday weekdays, Employed ## 5 Avg hrs per day for participants - Sleeping, Nonholiday weekdays, Employed ## 6 Avg hrs per day for participants - Sleeping, Nonholiday weekdays, Employed Finally, let’s filter that to exclude the ‘participants only’ group and only get the Men/Women values (not the combined totals): series_defn %&gt;% select(series_title) %&gt;% filter(grepl(&quot;per day -.*sleep.*nonholiday weekdays.*employed full time.*on days worked,&quot;, series_title, ignore.case = TRUE)) ## # A tibble: 2 × 1 ## series_title ## &lt;chr&gt; ## 1 Avg hrs per day - Sleeping, Nonholiday weekdays, Employed full time, on day ## 2 Avg hrs per day - Sleeping, Nonholiday weekdays, Employed full time, on day 8.1.3 Adding more activity categories Now let’s add ‘work’ and ‘leisure’ to our search: activity &lt;- series_defn %&gt;% select(series_id, series_title) %&gt;% filter(grepl(&quot;per day -.*(sleep|work|leisure).*nonholiday weekdays.*employed full time.*on days worked,&quot;, series_title, ignore.case = TRUE)) activity ## # A tibble: 26 × 2 ## series_id ## &lt;chr&gt; ## 1 TUU10101AA01000344 ## 2 TUU10101AA01000423 ## 3 TUU10101AA01000962 ## 4 TUU10101AA01001041 ## 5 TUU10101AA01003012 ## 6 TUU10101AA01003097 ## 7 TUU10101AA01003307 ## 8 TUU10101AA01003378 ## 9 TUU10101AA01003947 ## 10 TUU10101AA01004011 ## # ... with 16 more rows, and 1 more variables: series_title &lt;chr&gt; Now we should create a variable that codes each of these as either work, sleep, or leisure: activity &lt;- activity %&gt;% mutate( activity_type = case_when( grepl(&quot;leisure&quot;, activity$series_title, ignore.case = TRUE) ~ &quot;Leisure&quot;, grepl(&quot;sleep&quot;, activity$series_title, ignore.case = TRUE) ~ &quot;Sleep&quot;, TRUE ~ &quot;Work&quot; ), sex = ifelse(grepl(&quot;Men&quot;, series_title), &quot;Men&quot;, &quot;Women&quot;) ) activity ## # A tibble: 26 × 4 ## series_id ## &lt;chr&gt; ## 1 TUU10101AA01000344 ## 2 TUU10101AA01000423 ## 3 TUU10101AA01000962 ## 4 TUU10101AA01001041 ## 5 TUU10101AA01003012 ## 6 TUU10101AA01003097 ## 7 TUU10101AA01003307 ## 8 TUU10101AA01003378 ## 9 TUU10101AA01003947 ## 10 TUU10101AA01004011 ## # ... with 16 more rows, and 3 more variables: series_title &lt;chr&gt;, ## # activity_type &lt;chr&gt;, sex &lt;chr&gt; Now we can join the activity data.frame with the current data and create time series of each activity type we created. data &lt;- read_tsv(&quot;data/tu/tu.data.0.Current&quot;) data &lt;- data %&gt;% inner_join(activity) %&gt;% group_by(year, sex, activity_type) %&gt;% summarize(hours = sum(as.numeric(value), na.rm = TRUE)) data ## Source: local data frame [84 x 4] ## Groups: year, sex [?] ## ## year sex activity_type hours ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2003 Men Leisure 8.49 ## 2 2003 Men Sleep 7.46 ## 3 2003 Men Work 19.26 ## 4 2003 Women Leisure 7.02 ## 5 2003 Women Sleep 7.65 ## 6 2003 Women Work 17.87 ## 7 2004 Men Leisure 8.66 ## 8 2004 Men Sleep 7.49 ## 9 2004 Men Work 18.92 ## 10 2004 Women Leisure 7.34 ## # ... with 74 more rows 8.2 geom_area geom_area is useful when components that naturally add to each other: library(ggplot2) ggplot(data, aes(year, hours, fill= activity_type)) + geom_area() + facet_wrap(~ sex) 8.3 geom_ribbon data %&gt;% ggplot(aes(x = year, group = sex, fill = activity_type)) + geom_ribbon(mapping = aes(ymin = -hours * (sex == &quot;Women&quot;), ymax = hours * (sex == &quot;Men&quot;)), data = . %&gt;% filter(activity_type == &quot;Work&quot;), alpha = 0.5) + geom_ribbon(mapping = aes(ymin = -hours * (sex == &quot;Women&quot;), ymax = hours * (sex == &quot;Men&quot;)), data = . %&gt;% filter(activity_type == &quot;Leisure&quot;), alpha = 0.5) + geom_ribbon(mapping = aes(ymin = -hours * (sex == &quot;Women&quot;), ymax = hours * (sex == &quot;Men&quot;)), data = . %&gt;% filter(activity_type == &quot;Sleep&quot;), alpha = 0.5) + scale_y_continuous( name = &quot;Average hours per work day (Fully Employed)&quot;, breaks = c(-20, -10, 0, 10, 20), labels = c(&quot;Women 20 hrs&quot;, &quot;10 hrs&quot;, &quot;0 hrs&quot;, &quot;10 hrs&quot;, &quot;Men 20 hrs&quot;), limits = c(-20, 20) ) "],
["data-sources.html", "Data Sources Overview Macro Data Micro Data Hawaii Data Collections of data lists", " Data Sources Overview While you can find many data sources by typing public data sources in your favorite search engine, the following lists should help you get started. Macro Data US Federal Reserve Economic Data (FRED) Bureau of Labor Statistics Bureau of Economic Analysis National Bureau of Economic Research Congressional Budget Office: Budget and Economic Data American FactFinder (American Community Survey, Census Summary Files, etc.) Other US The Conference Board (includes consumer confidence index) Survey of Consumers Historical Exchange Rates Center for Medicare and Medicaid Services Micro Data Panel Study on Income Dynamics IPUMS (census and survey data) US Census Public Use Microdata Sample (PUMS) Center for Medicare and Medicaid Services Hawaii Data State of Hawaii Department of Business, Economic Development and Tourism (DBEDT) Collections of data lists American Economic Association (AEA) list of data sources "],
["anscombe.html", "Anscombe’s Quartet Prep the data Numeric summary Visual summary The Datasaurus Dozen", " Anscombe’s Quartet Anscombe quartet emphasizes the need to move beyond basic numerical summaries of your data. The anscombe dataset has four sets of x and y variables with very similar summaries, but distinct visual patterns Prep the data anscombe ## x1 x2 x3 x4 y1 y2 y3 y4 ## 1 10 10 10 8 8.04 9.14 7.46 6.58 ## 2 8 8 8 8 6.95 8.14 6.77 5.76 ## 3 13 13 13 8 7.58 8.74 12.74 7.71 ## 4 9 9 9 8 8.81 8.77 7.11 8.84 ## 5 11 11 11 8 8.33 9.26 7.81 8.47 ## 6 14 14 14 8 9.96 8.10 8.84 7.04 ## 7 6 6 6 8 7.24 6.13 6.08 5.25 ## 8 4 4 4 19 4.26 3.10 5.39 12.50 ## 9 12 12 12 8 10.84 9.13 8.15 5.56 ## 10 7 7 7 8 4.82 7.26 6.42 7.91 ## 11 5 5 5 8 5.68 4.74 5.73 6.89 First we’ll use tidyr to reshape the anscombe dataset to make it easier to work with. library(tidyr) tidy_anscombe &lt;- anscombe %&gt;% mutate(id = row_number()) %&gt;% gather(key = key, value = value, everything(), -id) tidy_anscombe ## id key value ## 1 1 x1 10.00 ## 2 2 x1 8.00 ## 3 3 x1 13.00 ## 4 4 x1 9.00 ## 5 5 x1 11.00 ## 6 6 x1 14.00 ## 7 7 x1 6.00 ## 8 8 x1 4.00 ## 9 9 x1 12.00 ## 10 10 x1 7.00 ## 11 11 x1 5.00 ## 12 1 x2 10.00 ## 13 2 x2 8.00 ## 14 3 x2 13.00 ## 15 4 x2 9.00 ## 16 5 x2 11.00 ## 17 6 x2 14.00 ## 18 7 x2 6.00 ## 19 8 x2 4.00 ## 20 9 x2 12.00 ## 21 10 x2 7.00 ## 22 11 x2 5.00 ## 23 1 x3 10.00 ## 24 2 x3 8.00 ## 25 3 x3 13.00 ## 26 4 x3 9.00 ## 27 5 x3 11.00 ## 28 6 x3 14.00 ## 29 7 x3 6.00 ## 30 8 x3 4.00 ## 31 9 x3 12.00 ## 32 10 x3 7.00 ## 33 11 x3 5.00 ## 34 1 x4 8.00 ## 35 2 x4 8.00 ## 36 3 x4 8.00 ## 37 4 x4 8.00 ## 38 5 x4 8.00 ## 39 6 x4 8.00 ## 40 7 x4 8.00 ## 41 8 x4 19.00 ## 42 9 x4 8.00 ## 43 10 x4 8.00 ## 44 11 x4 8.00 ## 45 1 y1 8.04 ## 46 2 y1 6.95 ## 47 3 y1 7.58 ## 48 4 y1 8.81 ## 49 5 y1 8.33 ## 50 6 y1 9.96 ## 51 7 y1 7.24 ## 52 8 y1 4.26 ## 53 9 y1 10.84 ## 54 10 y1 4.82 ## 55 11 y1 5.68 ## 56 1 y2 9.14 ## 57 2 y2 8.14 ## 58 3 y2 8.74 ## 59 4 y2 8.77 ## 60 5 y2 9.26 ## 61 6 y2 8.10 ## 62 7 y2 6.13 ## 63 8 y2 3.10 ## 64 9 y2 9.13 ## 65 10 y2 7.26 ## 66 11 y2 4.74 ## 67 1 y3 7.46 ## 68 2 y3 6.77 ## 69 3 y3 12.74 ## 70 4 y3 7.11 ## 71 5 y3 7.81 ## 72 6 y3 8.84 ## 73 7 y3 6.08 ## 74 8 y3 5.39 ## 75 9 y3 8.15 ## 76 10 y3 6.42 ## 77 11 y3 5.73 ## 78 1 y4 6.58 ## 79 2 y4 5.76 ## 80 3 y4 7.71 ## 81 4 y4 8.84 ## 82 5 y4 8.47 ## 83 6 y4 7.04 ## 84 7 y4 5.25 ## 85 8 y4 12.50 ## 86 9 y4 5.56 ## 87 10 y4 7.91 ## 88 11 y4 6.89 tidy_anscombe &lt;- tidy_anscombe %&gt;% separate(key, c(&quot;x_or_y&quot;, &quot;series&quot;), 1) tidy_anscombe ## id x_or_y series value ## 1 1 x 1 10.00 ## 2 2 x 1 8.00 ## 3 3 x 1 13.00 ## 4 4 x 1 9.00 ## 5 5 x 1 11.00 ## 6 6 x 1 14.00 ## 7 7 x 1 6.00 ## 8 8 x 1 4.00 ## 9 9 x 1 12.00 ## 10 10 x 1 7.00 ## 11 11 x 1 5.00 ## 12 1 x 2 10.00 ## 13 2 x 2 8.00 ## 14 3 x 2 13.00 ## 15 4 x 2 9.00 ## 16 5 x 2 11.00 ## 17 6 x 2 14.00 ## 18 7 x 2 6.00 ## 19 8 x 2 4.00 ## 20 9 x 2 12.00 ## 21 10 x 2 7.00 ## 22 11 x 2 5.00 ## 23 1 x 3 10.00 ## 24 2 x 3 8.00 ## 25 3 x 3 13.00 ## 26 4 x 3 9.00 ## 27 5 x 3 11.00 ## 28 6 x 3 14.00 ## 29 7 x 3 6.00 ## 30 8 x 3 4.00 ## 31 9 x 3 12.00 ## 32 10 x 3 7.00 ## 33 11 x 3 5.00 ## 34 1 x 4 8.00 ## 35 2 x 4 8.00 ## 36 3 x 4 8.00 ## 37 4 x 4 8.00 ## 38 5 x 4 8.00 ## 39 6 x 4 8.00 ## 40 7 x 4 8.00 ## 41 8 x 4 19.00 ## 42 9 x 4 8.00 ## 43 10 x 4 8.00 ## 44 11 x 4 8.00 ## 45 1 y 1 8.04 ## 46 2 y 1 6.95 ## 47 3 y 1 7.58 ## 48 4 y 1 8.81 ## 49 5 y 1 8.33 ## 50 6 y 1 9.96 ## 51 7 y 1 7.24 ## 52 8 y 1 4.26 ## 53 9 y 1 10.84 ## 54 10 y 1 4.82 ## 55 11 y 1 5.68 ## 56 1 y 2 9.14 ## 57 2 y 2 8.14 ## 58 3 y 2 8.74 ## 59 4 y 2 8.77 ## 60 5 y 2 9.26 ## 61 6 y 2 8.10 ## 62 7 y 2 6.13 ## 63 8 y 2 3.10 ## 64 9 y 2 9.13 ## 65 10 y 2 7.26 ## 66 11 y 2 4.74 ## 67 1 y 3 7.46 ## 68 2 y 3 6.77 ## 69 3 y 3 12.74 ## 70 4 y 3 7.11 ## 71 5 y 3 7.81 ## 72 6 y 3 8.84 ## 73 7 y 3 6.08 ## 74 8 y 3 5.39 ## 75 9 y 3 8.15 ## 76 10 y 3 6.42 ## 77 11 y 3 5.73 ## 78 1 y 4 6.58 ## 79 2 y 4 5.76 ## 80 3 y 4 7.71 ## 81 4 y 4 8.84 ## 82 5 y 4 8.47 ## 83 6 y 4 7.04 ## 84 7 y 4 5.25 ## 85 8 y 4 12.50 ## 86 9 y 4 5.56 ## 87 10 y 4 7.91 ## 88 11 y 4 6.89 tidy_anscombe &lt;- tidy_anscombe %&gt;% spread(x_or_y, value) tidy_anscombe ## id series x y ## 1 1 1 10 8.04 ## 2 1 2 10 9.14 ## 3 1 3 10 7.46 ## 4 1 4 8 6.58 ## 5 2 1 8 6.95 ## 6 2 2 8 8.14 ## 7 2 3 8 6.77 ## 8 2 4 8 5.76 ## 9 3 1 13 7.58 ## 10 3 2 13 8.74 ## 11 3 3 13 12.74 ## 12 3 4 8 7.71 ## 13 4 1 9 8.81 ## 14 4 2 9 8.77 ## 15 4 3 9 7.11 ## 16 4 4 8 8.84 ## 17 5 1 11 8.33 ## 18 5 2 11 9.26 ## 19 5 3 11 7.81 ## 20 5 4 8 8.47 ## 21 6 1 14 9.96 ## 22 6 2 14 8.10 ## 23 6 3 14 8.84 ## 24 6 4 8 7.04 ## 25 7 1 6 7.24 ## 26 7 2 6 6.13 ## 27 7 3 6 6.08 ## 28 7 4 8 5.25 ## 29 8 1 4 4.26 ## 30 8 2 4 3.10 ## 31 8 3 4 5.39 ## 32 8 4 19 12.50 ## 33 9 1 12 10.84 ## 34 9 2 12 9.13 ## 35 9 3 12 8.15 ## 36 9 4 8 5.56 ## 37 10 1 7 4.82 ## 38 10 2 7 7.26 ## 39 10 3 7 6.42 ## 40 10 4 8 7.91 ## 41 11 1 5 5.68 ## 42 11 2 5 4.74 ## 43 11 3 5 5.73 ## 44 11 4 8 6.89 Numeric summary tidy_anscombe %&gt;% group_by(series) %&gt;% summarise( mean_x = mean(x), mean_y = mean(y), sd_x = sd(x), sd_y = sd(y), cor = cor(x, y) ) ## # A tibble: 4 × 6 ## series mean_x mean_y sd_x sd_y cor ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 9 7.500909 3.316625 2.031568 0.8164205 ## 2 2 9 7.500909 3.316625 2.031657 0.8162365 ## 3 3 9 7.500000 3.316625 2.030424 0.8162867 ## 4 4 9 7.500909 3.316625 2.030579 0.8165214 Visual summary While the visual summaries suggest very similar datasets, the visual summaries help identify the differences: library(ggplot2) tidy_anscombe %&gt;% ggplot(aes(x, y)) + geom_point() + facet_wrap(~ series) + coord_fixed() The Datasaurus Dozen The Datasaurus Dozen is a set of series, like the Anscombe’s quartet with similar numerical summaries and radically different visual summaries. See a great discussion of this dataset by the creators, Justin Matejka and George Fitzmaurice here Download the data here and move the DatasaurusDozen.tsv file into your data folder. datasaurus &lt;- read_tsv(&quot;data/DatasaurusDozen.tsv&quot;) datasaurus %&gt;% group_by(dataset) %&gt;% summarise( mean_x = mean(x), mean_y = mean(y), sd_x = sd(x), sd_y = sd(y), cor = cor(x, y) ) ## # A tibble: 13 × 6 ## dataset mean_x mean_y sd_x sd_y cor ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 away 54.26610 47.83472 16.76982 26.93974 -0.06412835 ## 2 bullseye 54.26873 47.83082 16.76924 26.93573 -0.06858639 ## 3 circle 54.26732 47.83772 16.76001 26.93004 -0.06834336 ## 4 dino 54.26327 47.83225 16.76514 26.93540 -0.06447185 ## 5 dots 54.26030 47.83983 16.76774 26.93019 -0.06034144 ## 6 h_lines 54.26144 47.83025 16.76590 26.93988 -0.06171484 ## 7 high_lines 54.26881 47.83545 16.76670 26.94000 -0.06850422 ## 8 slant_down 54.26785 47.83590 16.76676 26.93610 -0.06897974 ## 9 slant_up 54.26588 47.83150 16.76885 26.93861 -0.06860921 ## 10 star 54.26734 47.83955 16.76896 26.93027 -0.06296110 ## 11 v_lines 54.26993 47.83699 16.76996 26.93768 -0.06944557 ## 12 wide_lines 54.26692 47.83160 16.77000 26.93790 -0.06657523 ## 13 x_shape 54.26015 47.83972 16.76996 26.93000 -0.06558334 Visual summaries datasaurus %&gt;% ggplot(aes(x, y)) + geom_point() + facet_wrap(~ dataset, ncol = 6) + coord_fixed() "],
["trifecta.html", "How to Judge Visualizations Pre-class assignment Lecture Assignment", " How to Judge Visualizations Pre-class assignment Read Junk Charts Trifecta Checkup Lecture Kaiser Fung has a useful framework for judging the quality of data visualizations. He calls his framework the Junk Charts Trifecta Checkup. To make that shorter, we’ll just call this framework the Trifecta Checkup. This framework boils down to the following 3 questions: What is the question? What does the data say? What does the visual say? Each visualization can be labeled according to which question(s) it does not answer well. For example, a chart with a meaningful question and relevant data, but ineffective visuals, would be labeled v. A chart that gets the data and question wrong, but has a nice visual, would be labeled qd. Assignment Assign a label to each of the following visualizations based on the Trifecta Checkup. Provide at least one sentence per question defending your chosen labels. "]
]
